{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009e8fc2",
   "metadata": {},
   "source": [
    "# 1. Build your own convolutional neural network using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ee2e8",
   "metadata": {},
   "source": [
    "Define the `NeuralNet` model, which is a customizable convolutional neural network designed for image classification tasks. \n",
    "\n",
    "It has a sequence of convolutional layers with optional pooling layers for feature extraction, followed by a series of fully connected layers for classification. We can adjust the number and configuration of hidden layers and pooling operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8fb46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ec4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_hiddens: List[int], \n",
    "        poolings: List[bool], \n",
    "        n_classes: int\n",
    "    ):\n",
    "        assert len(n_hiddens) == len(poolings)\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_hiddens: List[int] = n_hiddens\n",
    "        self.poolings: List[bool] = poolings\n",
    "        self.n_classes: int = n_classes\n",
    "\n",
    "        feature_extractor_modules: List[nn.Module] = []\n",
    "        for n_hidden, pooling in zip(n_hiddens, poolings):\n",
    "            feature_extractor_modules.extend([\n",
    "                nn.LazyConv2d(out_channels=n_hidden, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(num_features=n_hidden),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            if pooling:\n",
    "                feature_extractor_modules.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor_modules)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1, end_dim=-1),\n",
    "            nn.LazyLinear(out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(out_features=n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.feature_extractor(x)\n",
    "        y = self.classifier(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4961e0f",
   "metadata": {},
   "source": [
    "# 2. Train your model using dog heart dataset (you may need to use  Google Colab (or Kaggle) with GPU to train your code) \n",
    "\n",
    "### (1) use torchvision.datasets.ImageFolder for the training dataset\n",
    "### (2) use custom dataloader for test dataset (return image tensor and file name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e73b2",
   "metadata": {},
   "source": [
    "### Utility classes:\n",
    "Define `Accumulator` class to track performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from typing import Optional, Dict, TextIO, Any\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b50215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"\n",
    "    A utility class for accumulating values for multiple metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.__records: defaultdict[str, float] = defaultdict(float)\n",
    "\n",
    "    def add(self, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Add values to the accumulator.\n",
    "\n",
    "        Parameters:\n",
    "            - **kwargs: named metric and the value is the amount to add.\n",
    "        \"\"\"\n",
    "        metric: str\n",
    "        value: float\n",
    "        for metric, value in kwargs.items():\n",
    "            # Each keyword argument represents a metric name and its value to be added\n",
    "            self.__records[metric] += value\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the accumulator by clearing all recorded metrics.\n",
    "        \"\"\"\n",
    "        self.__records.clear()\n",
    "\n",
    "    def __getitem__(self, key: str) -> float:\n",
    "        \"\"\"\n",
    "        Retrieve a record by key.\n",
    "\n",
    "        Parameters:\n",
    "            - key (str): The record key name.\n",
    "\n",
    "        Returns:\n",
    "            - float: The record value.\n",
    "        \"\"\"\n",
    "        return self.__records[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67036d3",
   "metadata": {},
   "source": [
    "Define `EarlyStopping` to early stop the training process given on some validation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267754d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    A simple early stopping utility to terminate training when a monitored metric stops improving.\n",
    "\n",
    "    Attributes:\n",
    "        - patience (int): The number of epochs with no improvement after which training will be stopped.\n",
    "        - tolerance (float): The minimum change in the monitored metric to qualify as an improvement,\n",
    "        - considering the direction of the metric being monitored.\n",
    "        - bestscore (float): The best score seen so far.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int, tolerance: float = 0.) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopping instance.\n",
    "        \n",
    "        Parameters:\n",
    "            - patience (int): Number of epochs with no improvement after which training will be stopped.\n",
    "            - tolerance (float): The minimum change in the monitored metric to qualify as an improvement. \n",
    "            Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.patience: int = patience\n",
    "        self.tolerance: float = tolerance\n",
    "        self.bestscore: float = float('inf')\n",
    "        self.__counter: int = 0\n",
    "\n",
    "    def __call__(self, value: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the state of the early stopping mechanism based on the new metric value.\n",
    "\n",
    "        Parameters:\n",
    "            - value (float): The latest value of the monitored metric.\n",
    "        \"\"\"\n",
    "        # Improvement or within tolerance, reset counter\n",
    "        if value <= self.bestscore + self.tolerance:\n",
    "            self.bestscore: float = value\n",
    "            self.__counter: int = 0\n",
    "\n",
    "        # No improvement, increment counter\n",
    "        else:\n",
    "            self.__counter += 1\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the training process should be stopped early.\n",
    "\n",
    "        Returns:\n",
    "            - bool: True if training should be stopped (patience exceeded), otherwise False.\n",
    "        \"\"\"\n",
    "        return self.__counter >= self.patience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427e8d5",
   "metadata": {},
   "source": [
    "Define `Logger` class to log the training process to file and console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df22a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    \"\"\"\n",
    "    A class used to log the training process.\n",
    "\n",
    "    This class provides methods to log messages to a file and the console. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        logfile: str = f\".log/{dt.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    ) -> None:\n",
    "    \n",
    "        \"\"\"\n",
    "        Initialize the logger.\n",
    "\n",
    "        Parameters:\n",
    "            - logfile (str, optional): The path to the logfile. \n",
    "            Defaults to a file in the .log directory with the current timestamp.\n",
    "        \"\"\"\n",
    "        self.logfile: pathlib.Path = pathlib.Path(logfile)\n",
    "        os.makedirs(name=self.logfile.parent, exist_ok=True)\n",
    "        self._file: TextIO = open(self.logfile, mode='w')\n",
    "\n",
    "    def log(\n",
    "        self, \n",
    "        epoch: int, \n",
    "        n_epochs: int, \n",
    "        batch: Optional[int] = None, \n",
    "        n_batches: Optional[int] = None, \n",
    "        took: Optional[float] = None, \n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log a message to console and a log file\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The current epoch.\n",
    "            - n_epochs (int): The total number of epochs.\n",
    "            - batch (int, optional): The current batch. Defaults to None.\n",
    "            - n_batches (int, optional): The total number of batches. Defaults to None.\n",
    "            - took (float, optional): The time it took to process the batch or epoch. Defaults to None.\n",
    "            - **kwargs: Additional metrics to log.\n",
    "        \"\"\"\n",
    "        suffix: str = ', '.join([f'{metric}: {value:.3e}' for metric, value in kwargs.items()])\n",
    "        prefix: str = f'Epoch {epoch}/{n_epochs} | '\n",
    "        if batch is not None:\n",
    "            prefix += f'Batch {batch}/{n_batches} | '\n",
    "        if took is not None:\n",
    "            prefix += f'Took {took:.2f}s | '\n",
    "        logstring: str = prefix + suffix\n",
    "        print(logstring)\n",
    "        self._file.write(logstring + '\\n')\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        \"\"\"\n",
    "        Close the logfile at garbage collected.\n",
    "        \"\"\"\n",
    "        self._file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a5228",
   "metadata": {},
   "source": [
    "Define `CheckPointSaver` to save model's checkpoints during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9841a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckPointSaver:\n",
    "    \"\"\"\n",
    "    A class used to save PyTorch model checkpoints.\n",
    "\n",
    "    Attributes:\n",
    "        - dirpath (pathlib.Path): The directory where the checkpoints are saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dirpath: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CheckPointSaver.\n",
    "\n",
    "        Parameters:\n",
    "            - dirpath (os.PathLike): The directory where the checkpoints are saved.\n",
    "        \"\"\"\n",
    "        self.dirpath: pathlib.Path = pathlib.Path(dirpath)\n",
    "        os.makedirs(name=self.dirpath, exist_ok=True)\n",
    "\n",
    "    def save(self, model: nn.Module, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Save checkpoint to a .pt file.\n",
    "\n",
    "        Parameters:\n",
    "            - model (nn.Module): The PyTorch model to save.\n",
    "            - filename (str): the checkpoint file name\n",
    "        \"\"\"\n",
    "        torch.save(obj=model, f=os.path.join(self.dirpath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29a5e4",
   "metadata": {},
   "source": [
    "### `Dataset` classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb1f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564831d5",
   "metadata": {},
   "source": [
    "Define `DogHeartLabeledDataset` class for labeled dataset (training and validation), this class extends the `ImageFolder` from `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5ad8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogHeartLabeledDataset(ImageFolder):\n",
    "\n",
    "    #extend\n",
    "    def __init__(self, data_root: str) -> None:\n",
    "        self.transformation = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Grayscale(),\n",
    "            torchvision.transforms.Resize((128, 128)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "        super().__init__(root=data_root, transform=self.transformation)\n",
    "        self.data_root: str = data_root\n",
    "\n",
    "        self.filepaths: List[str] = [path for path, _ in self.samples]\n",
    "        self.filenames: List[str] = [path.split('/')[-1] for path in self.filepaths]\n",
    "        self.labels: List[int] = [label for _, label in self.samples]\n",
    "\n",
    "    #extend\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, str]:\n",
    "        tensor: torch.Tensor; label: int\n",
    "        tensor, label = super().__getitem__(idx)\n",
    "        filename: str = self.filenames[idx]\n",
    "        return tensor, label, filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78dbee",
   "metadata": {},
   "source": [
    "Define `DogHeartUnlabeledDataset` class for unlabeled dataset (testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56607d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogHearUnlabeledDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_root: str) -> None:\n",
    "        self.data_root: str = data_root\n",
    "        self.transformation = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Grayscale(),\n",
    "            torchvision.transforms.Resize((128, 128)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "        self.filenames: List[str] = os.listdir(self.data_root)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, str]:\n",
    "        filename: str = self.filenames[idx]\n",
    "        image: Image = Image.open(os.path.join(self.data_root, filename))\n",
    "        tensor: torch.Tensor = self.transformation(image)\n",
    "        return tensor, filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fcd66",
   "metadata": {},
   "source": [
    "Create dataloaders from labeled datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed68234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DogHeartLabeledDataset(data_root='Dog_heart/Train')\n",
    "valid_dataset = DogHeartLabeledDataset(data_root='Dog_heart/Valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97efd80a",
   "metadata": {},
   "source": [
    "Define the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd24f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac3cd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(\n",
    "    scores: torch.Tensor,\n",
    "    gt_labels: torch.Tensor,\n",
    "):\n",
    "    return F.cross_entropy(input=scores, target=gt_labels, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92701546",
   "metadata": {},
   "source": [
    "Specify the computing device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb8b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0807991",
   "metadata": {},
   "source": [
    "Define the `evaluation` function to report the accuracy and data loss over a batched dataset (dataloader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdce575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, dataloader: DataLoader) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    metrics = Accumulator()\n",
    "\n",
    "    # Loop through each batch\n",
    "    for batch, (batch_images, gt_labels, filenames) in enumerate(dataloader, start=1):\n",
    "        batch_images = batch_images.to(device)\n",
    "        gt_labels = gt_labels.to(device)\n",
    "        scores: torch.Tensor = model(batch_images)\n",
    "        pred_labels = scores.max(dim=1).indices\n",
    "        n_corrects = (pred_labels == gt_labels).sum().item()\n",
    "        n_predictions = pred_labels.numel()\n",
    "        loss = loss_function(scores, gt_labels).mean()\n",
    "\n",
    "        # Accumulate the metrics\n",
    "        metrics.add(n_corrects=n_corrects, n_predictions=n_predictions, loss=loss.item())\n",
    "\n",
    "    # Compute the aggregate metrics\n",
    "    accuracy: float = metrics['n_corrects'] / metrics['n_predictions']\n",
    "    loss: float = metrics['loss'] / batch\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87249365",
   "metadata": {},
   "source": [
    "Define the `train` function that implements the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe0e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    n_epochs: int,\n",
    "    patience: int,\n",
    "    tolerance: float,\n",
    "    checkpoint_dir: Optional[str] = None,\n",
    ") -> nn.Module:\n",
    "\n",
    "    model.train()\n",
    "    train_metrics = Accumulator()\n",
    "    early_stopping = EarlyStopping(patience, tolerance)\n",
    "    logger = Logger()\n",
    "    checkpoint_saver = CheckPointSaver(dirpath=checkpoint_dir)\n",
    "\n",
    "    # loop through each epoch\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Loop through each batch\n",
    "        for batch, (batch_images, gt_labels, filenames) in enumerate(train_dataloader, start=1):\n",
    "            batch_images = batch_images.to(device)\n",
    "            gt_labels = gt_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            scores: torch.Tensor = model(batch_images)\n",
    "            pred_labels: torch.Tensor = scores.max(dim=1).indices\n",
    "            # print(pred_labels.detach().cpu().numpy())\n",
    "            # print(gt_labels.detach().cpu().numpy())\n",
    "            n_corrects: int = (pred_labels == gt_labels).sum().item()\n",
    "            n_predictions: int = pred_labels.numel()\n",
    "            loss: torch.Tensor = loss_function(scores, gt_labels).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate the metrics\n",
    "            train_metrics.add(n_correct=n_corrects, n_predictions=n_predictions, loss=loss.item())\n",
    "            train_accuracy: float = train_metrics['n_correct'] / train_metrics['n_predictions']\n",
    "            train_loss = train_metrics['loss'] / batch\n",
    "            logger.log(\n",
    "                epoch=epoch, n_epochs=n_epochs, batch=batch, n_batches=len(train_dataloader),\n",
    "                train_accuracy=train_accuracy, train_loss=train_loss\n",
    "            )\n",
    "\n",
    "        # Save checkpoint\n",
    "        if checkpoint_dir:\n",
    "            checkpoint_saver.save(model, filename=f'epoch{epoch}.pt')\n",
    "\n",
    "        # Reset metric records for next epoch\n",
    "        train_metrics.reset()\n",
    "\n",
    "        # Evaluate\n",
    "        val_accuracy, val_loss = evaluate(model=model, dataloader=val_dataloader)\n",
    "        logger.log(epoch=epoch, n_epochs=n_epochs, val_accuracy=val_accuracy, val_loss=val_loss)\n",
    "        print('='*20)\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping:\n",
    "            print('Early Stopped')\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda634a",
   "metadata": {},
   "source": [
    "Create an instance of `NeuralNet`, which is a Convolutional Neural Network. \n",
    "\n",
    "Its feature extractor consists of 9 convolutional layers with hidden dimensions progressively increasing from 512 to 2048. Pooling layers are applied after every third convolutional layer to reduce spatial dimensions. \n",
    "\n",
    "The feature extractor is followed by a classifier with fully connected layers predicting three output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99ee3acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/katz_dnn_project_one/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNet(\n",
    "    n_hiddens=[\n",
    "        512, 512, 512, \n",
    "        1024, 1024, 1024, \n",
    "        2048, 2048, 2048,\n",
    "    ], \n",
    "    poolings=[\n",
    "        True, False, False, \n",
    "        True, False, False, \n",
    "        True, False, False,\n",
    "    ],\n",
    "    n_classes=3,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5a858",
   "metadata": {},
   "source": [
    "Create an instance of Adam optimizer, which has the learning rate adaptively changing from `0.00001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d0ec9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d500c0",
   "metadata": {},
   "source": [
    "Now we run the training loop on the maximum number of `100` epochs. \n",
    "\n",
    "After each epoch, the model is evaluated on the validation dataset. The early stopping is implemented to stop the training process if the validation loss does not improve after `10` consecutive epochs. The tolerance of the improvement is set to `0`. This early stopping helps avoid overfitting.\n",
    "\n",
    "The checkpoints are saved after each epoch in the `.pt` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "516a897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Batch 1/88 | train_accuracy: 5.000e-01, train_loss: 1.077e+00\n",
      "Epoch 1/100 | Batch 2/88 | train_accuracy: 5.000e-01, train_loss: 1.166e+00\n",
      "Epoch 1/100 | Batch 3/88 | train_accuracy: 5.625e-01, train_loss: 1.014e+00\n",
      "Epoch 1/100 | Batch 4/88 | train_accuracy: 4.844e-01, train_loss: 1.288e+00\n",
      "Epoch 1/100 | Batch 5/88 | train_accuracy: 4.500e-01, train_loss: 1.378e+00\n",
      "Epoch 1/100 | Batch 6/88 | train_accuracy: 4.167e-01, train_loss: 1.360e+00\n",
      "Epoch 1/100 | Batch 7/88 | train_accuracy: 4.196e-01, train_loss: 1.319e+00\n",
      "Epoch 1/100 | Batch 8/88 | train_accuracy: 3.984e-01, train_loss: 1.292e+00\n",
      "Epoch 1/100 | Batch 9/88 | train_accuracy: 3.889e-01, train_loss: 1.280e+00\n",
      "Epoch 1/100 | Batch 10/88 | train_accuracy: 3.812e-01, train_loss: 1.278e+00\n",
      "Epoch 1/100 | Batch 11/88 | train_accuracy: 3.920e-01, train_loss: 1.255e+00\n",
      "Epoch 1/100 | Batch 12/88 | train_accuracy: 3.802e-01, train_loss: 1.225e+00\n",
      "Epoch 1/100 | Batch 13/88 | train_accuracy: 3.846e-01, train_loss: 1.210e+00\n",
      "Epoch 1/100 | Batch 14/88 | train_accuracy: 3.973e-01, train_loss: 1.185e+00\n",
      "Epoch 1/100 | Batch 15/88 | train_accuracy: 3.958e-01, train_loss: 1.187e+00\n",
      "Epoch 1/100 | Batch 16/88 | train_accuracy: 3.906e-01, train_loss: 1.198e+00\n",
      "Epoch 1/100 | Batch 17/88 | train_accuracy: 4.007e-01, train_loss: 1.178e+00\n",
      "Epoch 1/100 | Batch 18/88 | train_accuracy: 3.958e-01, train_loss: 1.169e+00\n",
      "Epoch 1/100 | Batch 19/88 | train_accuracy: 4.013e-01, train_loss: 1.157e+00\n",
      "Epoch 1/100 | Batch 20/88 | train_accuracy: 4.125e-01, train_loss: 1.146e+00\n",
      "Epoch 1/100 | Batch 21/88 | train_accuracy: 4.167e-01, train_loss: 1.143e+00\n",
      "Epoch 1/100 | Batch 22/88 | train_accuracy: 4.205e-01, train_loss: 1.135e+00\n",
      "Epoch 1/100 | Batch 23/88 | train_accuracy: 4.239e-01, train_loss: 1.126e+00\n",
      "Epoch 1/100 | Batch 24/88 | train_accuracy: 4.349e-01, train_loss: 1.109e+00\n",
      "Epoch 1/100 | Batch 25/88 | train_accuracy: 4.275e-01, train_loss: 1.110e+00\n",
      "Epoch 1/100 | Batch 26/88 | train_accuracy: 4.327e-01, train_loss: 1.096e+00\n",
      "Epoch 1/100 | Batch 27/88 | train_accuracy: 4.398e-01, train_loss: 1.090e+00\n",
      "Epoch 1/100 | Batch 28/88 | train_accuracy: 4.464e-01, train_loss: 1.083e+00\n",
      "Epoch 1/100 | Batch 29/88 | train_accuracy: 4.483e-01, train_loss: 1.076e+00\n",
      "Epoch 1/100 | Batch 30/88 | train_accuracy: 4.417e-01, train_loss: 1.089e+00\n",
      "Epoch 1/100 | Batch 31/88 | train_accuracy: 4.435e-01, train_loss: 1.086e+00\n",
      "Epoch 1/100 | Batch 32/88 | train_accuracy: 4.492e-01, train_loss: 1.074e+00\n",
      "Epoch 1/100 | Batch 33/88 | train_accuracy: 4.564e-01, train_loss: 1.065e+00\n",
      "Epoch 1/100 | Batch 34/88 | train_accuracy: 4.596e-01, train_loss: 1.060e+00\n",
      "Epoch 1/100 | Batch 35/88 | train_accuracy: 4.625e-01, train_loss: 1.056e+00\n",
      "Epoch 1/100 | Batch 36/88 | train_accuracy: 4.618e-01, train_loss: 1.054e+00\n",
      "Epoch 1/100 | Batch 37/88 | train_accuracy: 4.595e-01, train_loss: 1.054e+00\n",
      "Epoch 1/100 | Batch 38/88 | train_accuracy: 4.556e-01, train_loss: 1.059e+00\n",
      "Epoch 1/100 | Batch 39/88 | train_accuracy: 4.551e-01, train_loss: 1.055e+00\n",
      "Epoch 1/100 | Batch 40/88 | train_accuracy: 4.562e-01, train_loss: 1.051e+00\n",
      "Epoch 1/100 | Batch 41/88 | train_accuracy: 4.604e-01, train_loss: 1.043e+00\n",
      "Epoch 1/100 | Batch 42/88 | train_accuracy: 4.643e-01, train_loss: 1.037e+00\n",
      "Epoch 1/100 | Batch 43/88 | train_accuracy: 4.680e-01, train_loss: 1.028e+00\n",
      "Epoch 1/100 | Batch 44/88 | train_accuracy: 4.659e-01, train_loss: 1.027e+00\n",
      "Epoch 1/100 | Batch 45/88 | train_accuracy: 4.681e-01, train_loss: 1.024e+00\n",
      "Epoch 1/100 | Batch 46/88 | train_accuracy: 4.592e-01, train_loss: 1.030e+00\n",
      "Epoch 1/100 | Batch 47/88 | train_accuracy: 4.614e-01, train_loss: 1.029e+00\n",
      "Epoch 1/100 | Batch 48/88 | train_accuracy: 4.609e-01, train_loss: 1.029e+00\n",
      "Epoch 1/100 | Batch 49/88 | train_accuracy: 4.630e-01, train_loss: 1.022e+00\n",
      "Epoch 1/100 | Batch 50/88 | train_accuracy: 4.675e-01, train_loss: 1.017e+00\n",
      "Epoch 1/100 | Batch 51/88 | train_accuracy: 4.694e-01, train_loss: 1.015e+00\n",
      "Epoch 1/100 | Batch 52/88 | train_accuracy: 4.712e-01, train_loss: 1.010e+00\n",
      "Epoch 1/100 | Batch 53/88 | train_accuracy: 4.693e-01, train_loss: 1.011e+00\n",
      "Epoch 1/100 | Batch 54/88 | train_accuracy: 4.711e-01, train_loss: 1.009e+00\n",
      "Epoch 1/100 | Batch 55/88 | train_accuracy: 4.739e-01, train_loss: 1.004e+00\n",
      "Epoch 1/100 | Batch 56/88 | train_accuracy: 4.710e-01, train_loss: 1.002e+00\n",
      "Epoch 1/100 | Batch 57/88 | train_accuracy: 4.748e-01, train_loss: 9.969e-01\n",
      "Epoch 1/100 | Batch 58/88 | train_accuracy: 4.774e-01, train_loss: 9.929e-01\n",
      "Epoch 1/100 | Batch 59/88 | train_accuracy: 4.820e-01, train_loss: 9.895e-01\n",
      "Epoch 1/100 | Batch 60/88 | train_accuracy: 4.854e-01, train_loss: 9.844e-01\n",
      "Epoch 1/100 | Batch 61/88 | train_accuracy: 4.867e-01, train_loss: 9.847e-01\n",
      "Epoch 1/100 | Batch 62/88 | train_accuracy: 4.879e-01, train_loss: 9.823e-01\n",
      "Epoch 1/100 | Batch 63/88 | train_accuracy: 4.911e-01, train_loss: 9.762e-01\n",
      "Epoch 1/100 | Batch 64/88 | train_accuracy: 4.941e-01, train_loss: 9.718e-01\n",
      "Epoch 1/100 | Batch 65/88 | train_accuracy: 4.981e-01, train_loss: 9.671e-01\n",
      "Epoch 1/100 | Batch 66/88 | train_accuracy: 5.000e-01, train_loss: 9.652e-01\n",
      "Epoch 1/100 | Batch 67/88 | train_accuracy: 5.028e-01, train_loss: 9.612e-01\n",
      "Epoch 1/100 | Batch 68/88 | train_accuracy: 5.018e-01, train_loss: 9.587e-01\n",
      "Epoch 1/100 | Batch 69/88 | train_accuracy: 5.000e-01, train_loss: 9.582e-01\n",
      "Epoch 1/100 | Batch 70/88 | train_accuracy: 5.009e-01, train_loss: 9.556e-01\n",
      "Epoch 1/100 | Batch 71/88 | train_accuracy: 5.035e-01, train_loss: 9.502e-01\n",
      "Epoch 1/100 | Batch 72/88 | train_accuracy: 5.043e-01, train_loss: 9.471e-01\n",
      "Epoch 1/100 | Batch 73/88 | train_accuracy: 5.060e-01, train_loss: 9.432e-01\n",
      "Epoch 1/100 | Batch 74/88 | train_accuracy: 5.076e-01, train_loss: 9.382e-01\n",
      "Epoch 1/100 | Batch 75/88 | train_accuracy: 5.092e-01, train_loss: 9.365e-01\n",
      "Epoch 1/100 | Batch 76/88 | train_accuracy: 5.107e-01, train_loss: 9.358e-01\n",
      "Epoch 1/100 | Batch 77/88 | train_accuracy: 5.154e-01, train_loss: 9.311e-01\n",
      "Epoch 1/100 | Batch 78/88 | train_accuracy: 5.176e-01, train_loss: 9.293e-01\n",
      "Epoch 1/100 | Batch 79/88 | train_accuracy: 5.158e-01, train_loss: 9.316e-01\n",
      "Epoch 1/100 | Batch 80/88 | train_accuracy: 5.180e-01, train_loss: 9.301e-01\n",
      "Epoch 1/100 | Batch 81/88 | train_accuracy: 5.185e-01, train_loss: 9.281e-01\n",
      "Epoch 1/100 | Batch 82/88 | train_accuracy: 5.198e-01, train_loss: 9.278e-01\n",
      "Epoch 1/100 | Batch 83/88 | train_accuracy: 5.181e-01, train_loss: 9.283e-01\n",
      "Epoch 1/100 | Batch 84/88 | train_accuracy: 5.186e-01, train_loss: 9.277e-01\n",
      "Epoch 1/100 | Batch 85/88 | train_accuracy: 5.199e-01, train_loss: 9.252e-01\n",
      "Epoch 1/100 | Batch 86/88 | train_accuracy: 5.211e-01, train_loss: 9.220e-01\n",
      "Epoch 1/100 | Batch 87/88 | train_accuracy: 5.208e-01, train_loss: 9.215e-01\n",
      "Epoch 1/100 | Batch 88/88 | train_accuracy: 5.214e-01, train_loss: 9.202e-01\n",
      "Epoch 1/100 | val_accuracy: 5.450e-01, val_loss: 8.100e-01\n",
      "====================\n",
      "Epoch 2/100 | Batch 1/88 | train_accuracy: 6.875e-01, train_loss: 6.170e-01\n",
      "Epoch 2/100 | Batch 2/88 | train_accuracy: 7.500e-01, train_loss: 5.646e-01\n",
      "Epoch 2/100 | Batch 3/88 | train_accuracy: 6.667e-01, train_loss: 6.732e-01\n",
      "Epoch 2/100 | Batch 4/88 | train_accuracy: 6.562e-01, train_loss: 6.662e-01\n",
      "Epoch 2/100 | Batch 5/88 | train_accuracy: 6.750e-01, train_loss: 6.559e-01\n",
      "Epoch 2/100 | Batch 6/88 | train_accuracy: 6.771e-01, train_loss: 6.523e-01\n",
      "Epoch 2/100 | Batch 7/88 | train_accuracy: 6.875e-01, train_loss: 6.428e-01\n",
      "Epoch 2/100 | Batch 8/88 | train_accuracy: 6.875e-01, train_loss: 6.672e-01\n",
      "Epoch 2/100 | Batch 9/88 | train_accuracy: 6.736e-01, train_loss: 6.787e-01\n",
      "Epoch 2/100 | Batch 10/88 | train_accuracy: 6.750e-01, train_loss: 6.982e-01\n",
      "Epoch 2/100 | Batch 11/88 | train_accuracy: 6.875e-01, train_loss: 6.819e-01\n",
      "Epoch 2/100 | Batch 12/88 | train_accuracy: 6.875e-01, train_loss: 6.690e-01\n",
      "Epoch 2/100 | Batch 13/88 | train_accuracy: 6.923e-01, train_loss: 6.600e-01\n",
      "Epoch 2/100 | Batch 14/88 | train_accuracy: 6.875e-01, train_loss: 6.676e-01\n",
      "Epoch 2/100 | Batch 15/88 | train_accuracy: 6.792e-01, train_loss: 6.739e-01\n",
      "Epoch 2/100 | Batch 16/88 | train_accuracy: 6.719e-01, train_loss: 6.764e-01\n",
      "Epoch 2/100 | Batch 17/88 | train_accuracy: 6.618e-01, train_loss: 7.022e-01\n",
      "Epoch 2/100 | Batch 18/88 | train_accuracy: 6.528e-01, train_loss: 7.137e-01\n",
      "Epoch 2/100 | Batch 19/88 | train_accuracy: 6.447e-01, train_loss: 7.253e-01\n",
      "Epoch 2/100 | Batch 20/88 | train_accuracy: 6.531e-01, train_loss: 7.132e-01\n",
      "Epoch 2/100 | Batch 21/88 | train_accuracy: 6.548e-01, train_loss: 7.078e-01\n",
      "Epoch 2/100 | Batch 22/88 | train_accuracy: 6.392e-01, train_loss: 7.190e-01\n",
      "Epoch 2/100 | Batch 23/88 | train_accuracy: 6.304e-01, train_loss: 7.340e-01\n",
      "Epoch 2/100 | Batch 24/88 | train_accuracy: 6.328e-01, train_loss: 7.333e-01\n",
      "Epoch 2/100 | Batch 25/88 | train_accuracy: 6.275e-01, train_loss: 7.326e-01\n",
      "Epoch 2/100 | Batch 26/88 | train_accuracy: 6.298e-01, train_loss: 7.298e-01\n",
      "Epoch 2/100 | Batch 27/88 | train_accuracy: 6.319e-01, train_loss: 7.416e-01\n",
      "Epoch 2/100 | Batch 28/88 | train_accuracy: 6.272e-01, train_loss: 7.448e-01\n",
      "Epoch 2/100 | Batch 29/88 | train_accuracy: 6.228e-01, train_loss: 7.510e-01\n",
      "Epoch 2/100 | Batch 30/88 | train_accuracy: 6.229e-01, train_loss: 7.490e-01\n",
      "Epoch 2/100 | Batch 31/88 | train_accuracy: 6.210e-01, train_loss: 7.507e-01\n",
      "Epoch 2/100 | Batch 32/88 | train_accuracy: 6.270e-01, train_loss: 7.450e-01\n",
      "Epoch 2/100 | Batch 33/88 | train_accuracy: 6.231e-01, train_loss: 7.486e-01\n",
      "Epoch 2/100 | Batch 34/88 | train_accuracy: 6.287e-01, train_loss: 7.457e-01\n",
      "Epoch 2/100 | Batch 35/88 | train_accuracy: 6.286e-01, train_loss: 7.451e-01\n",
      "Epoch 2/100 | Batch 36/88 | train_accuracy: 6.267e-01, train_loss: 7.455e-01\n",
      "Epoch 2/100 | Batch 37/88 | train_accuracy: 6.267e-01, train_loss: 7.448e-01\n",
      "Epoch 2/100 | Batch 38/88 | train_accuracy: 6.266e-01, train_loss: 7.487e-01\n",
      "Epoch 2/100 | Batch 39/88 | train_accuracy: 6.282e-01, train_loss: 7.458e-01\n",
      "Epoch 2/100 | Batch 40/88 | train_accuracy: 6.312e-01, train_loss: 7.432e-01\n",
      "Epoch 2/100 | Batch 41/88 | train_accuracy: 6.311e-01, train_loss: 7.443e-01\n",
      "Epoch 2/100 | Batch 42/88 | train_accuracy: 6.295e-01, train_loss: 7.448e-01\n",
      "Epoch 2/100 | Batch 43/88 | train_accuracy: 6.323e-01, train_loss: 7.445e-01\n",
      "Epoch 2/100 | Batch 44/88 | train_accuracy: 6.307e-01, train_loss: 7.423e-01\n",
      "Epoch 2/100 | Batch 45/88 | train_accuracy: 6.319e-01, train_loss: 7.391e-01\n",
      "Epoch 2/100 | Batch 46/88 | train_accuracy: 6.372e-01, train_loss: 7.337e-01\n",
      "Epoch 2/100 | Batch 47/88 | train_accuracy: 6.370e-01, train_loss: 7.318e-01\n",
      "Epoch 2/100 | Batch 48/88 | train_accuracy: 6.328e-01, train_loss: 7.403e-01\n",
      "Epoch 2/100 | Batch 49/88 | train_accuracy: 6.365e-01, train_loss: 7.360e-01\n",
      "Epoch 2/100 | Batch 50/88 | train_accuracy: 6.350e-01, train_loss: 7.432e-01\n",
      "Epoch 2/100 | Batch 51/88 | train_accuracy: 6.336e-01, train_loss: 7.432e-01\n",
      "Epoch 2/100 | Batch 52/88 | train_accuracy: 6.310e-01, train_loss: 7.454e-01\n",
      "Epoch 2/100 | Batch 53/88 | train_accuracy: 6.356e-01, train_loss: 7.417e-01\n",
      "Epoch 2/100 | Batch 54/88 | train_accuracy: 6.400e-01, train_loss: 7.366e-01\n",
      "Epoch 2/100 | Batch 55/88 | train_accuracy: 6.432e-01, train_loss: 7.317e-01\n",
      "Epoch 2/100 | Batch 56/88 | train_accuracy: 6.440e-01, train_loss: 7.331e-01\n",
      "Epoch 2/100 | Batch 57/88 | train_accuracy: 6.382e-01, train_loss: 7.392e-01\n",
      "Epoch 2/100 | Batch 58/88 | train_accuracy: 6.412e-01, train_loss: 7.356e-01\n",
      "Epoch 2/100 | Batch 59/88 | train_accuracy: 6.398e-01, train_loss: 7.351e-01\n",
      "Epoch 2/100 | Batch 60/88 | train_accuracy: 6.406e-01, train_loss: 7.333e-01\n",
      "Epoch 2/100 | Batch 61/88 | train_accuracy: 6.424e-01, train_loss: 7.314e-01\n",
      "Epoch 2/100 | Batch 62/88 | train_accuracy: 6.442e-01, train_loss: 7.276e-01\n",
      "Epoch 2/100 | Batch 63/88 | train_accuracy: 6.399e-01, train_loss: 7.349e-01\n",
      "Epoch 2/100 | Batch 64/88 | train_accuracy: 6.387e-01, train_loss: 7.367e-01\n",
      "Epoch 2/100 | Batch 65/88 | train_accuracy: 6.385e-01, train_loss: 7.368e-01\n",
      "Epoch 2/100 | Batch 66/88 | train_accuracy: 6.383e-01, train_loss: 7.353e-01\n",
      "Epoch 2/100 | Batch 67/88 | train_accuracy: 6.371e-01, train_loss: 7.347e-01\n",
      "Epoch 2/100 | Batch 68/88 | train_accuracy: 6.388e-01, train_loss: 7.335e-01\n",
      "Epoch 2/100 | Batch 69/88 | train_accuracy: 6.422e-01, train_loss: 7.314e-01\n",
      "Epoch 2/100 | Batch 70/88 | train_accuracy: 6.411e-01, train_loss: 7.329e-01\n",
      "Epoch 2/100 | Batch 71/88 | train_accuracy: 6.417e-01, train_loss: 7.332e-01\n",
      "Epoch 2/100 | Batch 72/88 | train_accuracy: 6.441e-01, train_loss: 7.296e-01\n",
      "Epoch 2/100 | Batch 73/88 | train_accuracy: 6.404e-01, train_loss: 7.324e-01\n",
      "Epoch 2/100 | Batch 74/88 | train_accuracy: 6.402e-01, train_loss: 7.358e-01\n",
      "Epoch 2/100 | Batch 75/88 | train_accuracy: 6.367e-01, train_loss: 7.399e-01\n",
      "Epoch 2/100 | Batch 76/88 | train_accuracy: 6.382e-01, train_loss: 7.385e-01\n",
      "Epoch 2/100 | Batch 77/88 | train_accuracy: 6.396e-01, train_loss: 7.361e-01\n",
      "Epoch 2/100 | Batch 78/88 | train_accuracy: 6.410e-01, train_loss: 7.327e-01\n",
      "Epoch 2/100 | Batch 79/88 | train_accuracy: 6.432e-01, train_loss: 7.293e-01\n",
      "Epoch 2/100 | Batch 80/88 | train_accuracy: 6.430e-01, train_loss: 7.289e-01\n",
      "Epoch 2/100 | Batch 81/88 | train_accuracy: 6.443e-01, train_loss: 7.282e-01\n",
      "Epoch 2/100 | Batch 82/88 | train_accuracy: 6.448e-01, train_loss: 7.301e-01\n",
      "Epoch 2/100 | Batch 83/88 | train_accuracy: 6.446e-01, train_loss: 7.287e-01\n",
      "Epoch 2/100 | Batch 84/88 | train_accuracy: 6.451e-01, train_loss: 7.287e-01\n",
      "Epoch 2/100 | Batch 85/88 | train_accuracy: 6.463e-01, train_loss: 7.261e-01\n",
      "Epoch 2/100 | Batch 86/88 | train_accuracy: 6.468e-01, train_loss: 7.255e-01\n",
      "Epoch 2/100 | Batch 87/88 | train_accuracy: 6.487e-01, train_loss: 7.227e-01\n",
      "Epoch 2/100 | Batch 88/88 | train_accuracy: 6.486e-01, train_loss: 7.224e-01\n",
      "Epoch 2/100 | val_accuracy: 6.700e-01, val_loss: 7.327e-01\n",
      "====================\n",
      "Epoch 3/100 | Batch 1/88 | train_accuracy: 7.500e-01, train_loss: 5.365e-01\n",
      "Epoch 3/100 | Batch 2/88 | train_accuracy: 7.500e-01, train_loss: 5.940e-01\n",
      "Epoch 3/100 | Batch 3/88 | train_accuracy: 7.083e-01, train_loss: 5.994e-01\n",
      "Epoch 3/100 | Batch 4/88 | train_accuracy: 7.031e-01, train_loss: 5.944e-01\n",
      "Epoch 3/100 | Batch 5/88 | train_accuracy: 7.125e-01, train_loss: 6.845e-01\n",
      "Epoch 3/100 | Batch 6/88 | train_accuracy: 7.500e-01, train_loss: 6.397e-01\n",
      "Epoch 3/100 | Batch 7/88 | train_accuracy: 6.875e-01, train_loss: 7.014e-01\n",
      "Epoch 3/100 | Batch 8/88 | train_accuracy: 6.953e-01, train_loss: 7.086e-01\n",
      "Epoch 3/100 | Batch 9/88 | train_accuracy: 7.014e-01, train_loss: 6.918e-01\n",
      "Epoch 3/100 | Batch 10/88 | train_accuracy: 7.000e-01, train_loss: 6.858e-01\n",
      "Epoch 3/100 | Batch 11/88 | train_accuracy: 7.159e-01, train_loss: 6.658e-01\n",
      "Epoch 3/100 | Batch 12/88 | train_accuracy: 7.135e-01, train_loss: 6.619e-01\n",
      "Epoch 3/100 | Batch 13/88 | train_accuracy: 6.971e-01, train_loss: 6.753e-01\n",
      "Epoch 3/100 | Batch 14/88 | train_accuracy: 6.920e-01, train_loss: 6.826e-01\n",
      "Epoch 3/100 | Batch 15/88 | train_accuracy: 7.000e-01, train_loss: 6.700e-01\n",
      "Epoch 3/100 | Batch 16/88 | train_accuracy: 6.875e-01, train_loss: 6.754e-01\n",
      "Epoch 3/100 | Batch 17/88 | train_accuracy: 6.985e-01, train_loss: 6.623e-01\n",
      "Epoch 3/100 | Batch 18/88 | train_accuracy: 6.910e-01, train_loss: 6.717e-01\n",
      "Epoch 3/100 | Batch 19/88 | train_accuracy: 6.842e-01, train_loss: 6.700e-01\n",
      "Epoch 3/100 | Batch 20/88 | train_accuracy: 6.875e-01, train_loss: 6.639e-01\n",
      "Epoch 3/100 | Batch 21/88 | train_accuracy: 6.756e-01, train_loss: 6.729e-01\n",
      "Epoch 3/100 | Batch 22/88 | train_accuracy: 6.761e-01, train_loss: 6.732e-01\n",
      "Epoch 3/100 | Batch 23/88 | train_accuracy: 6.766e-01, train_loss: 6.740e-01\n",
      "Epoch 3/100 | Batch 24/88 | train_accuracy: 6.823e-01, train_loss: 6.652e-01\n",
      "Epoch 3/100 | Batch 25/88 | train_accuracy: 6.875e-01, train_loss: 6.567e-01\n",
      "Epoch 3/100 | Batch 26/88 | train_accuracy: 6.899e-01, train_loss: 6.504e-01\n",
      "Epoch 3/100 | Batch 27/88 | train_accuracy: 6.898e-01, train_loss: 6.528e-01\n",
      "Epoch 3/100 | Batch 28/88 | train_accuracy: 6.875e-01, train_loss: 6.566e-01\n",
      "Epoch 3/100 | Batch 29/88 | train_accuracy: 6.875e-01, train_loss: 6.525e-01\n",
      "Epoch 3/100 | Batch 30/88 | train_accuracy: 6.854e-01, train_loss: 6.529e-01\n",
      "Epoch 3/100 | Batch 31/88 | train_accuracy: 6.895e-01, train_loss: 6.473e-01\n",
      "Epoch 3/100 | Batch 32/88 | train_accuracy: 6.895e-01, train_loss: 6.457e-01\n",
      "Epoch 3/100 | Batch 33/88 | train_accuracy: 6.894e-01, train_loss: 6.477e-01\n",
      "Epoch 3/100 | Batch 34/88 | train_accuracy: 6.857e-01, train_loss: 6.481e-01\n",
      "Epoch 3/100 | Batch 35/88 | train_accuracy: 6.804e-01, train_loss: 6.526e-01\n",
      "Epoch 3/100 | Batch 36/88 | train_accuracy: 6.892e-01, train_loss: 6.461e-01\n",
      "Epoch 3/100 | Batch 37/88 | train_accuracy: 6.841e-01, train_loss: 6.473e-01\n",
      "Epoch 3/100 | Batch 38/88 | train_accuracy: 6.809e-01, train_loss: 6.478e-01\n",
      "Epoch 3/100 | Batch 39/88 | train_accuracy: 6.747e-01, train_loss: 6.589e-01\n",
      "Epoch 3/100 | Batch 40/88 | train_accuracy: 6.719e-01, train_loss: 6.587e-01\n",
      "Epoch 3/100 | Batch 41/88 | train_accuracy: 6.707e-01, train_loss: 6.555e-01\n",
      "Epoch 3/100 | Batch 42/88 | train_accuracy: 6.771e-01, train_loss: 6.491e-01\n",
      "Epoch 3/100 | Batch 43/88 | train_accuracy: 6.773e-01, train_loss: 6.482e-01\n",
      "Epoch 3/100 | Batch 44/88 | train_accuracy: 6.790e-01, train_loss: 6.463e-01\n",
      "Epoch 3/100 | Batch 45/88 | train_accuracy: 6.792e-01, train_loss: 6.462e-01\n",
      "Epoch 3/100 | Batch 46/88 | train_accuracy: 6.780e-01, train_loss: 6.511e-01\n",
      "Epoch 3/100 | Batch 47/88 | train_accuracy: 6.822e-01, train_loss: 6.472e-01\n",
      "Epoch 3/100 | Batch 48/88 | train_accuracy: 6.862e-01, train_loss: 6.433e-01\n",
      "Epoch 3/100 | Batch 49/88 | train_accuracy: 6.837e-01, train_loss: 6.472e-01\n",
      "Epoch 3/100 | Batch 50/88 | train_accuracy: 6.813e-01, train_loss: 6.516e-01\n",
      "Epoch 3/100 | Batch 51/88 | train_accuracy: 6.814e-01, train_loss: 6.520e-01\n",
      "Epoch 3/100 | Batch 52/88 | train_accuracy: 6.803e-01, train_loss: 6.511e-01\n",
      "Epoch 3/100 | Batch 53/88 | train_accuracy: 6.840e-01, train_loss: 6.488e-01\n",
      "Epoch 3/100 | Batch 54/88 | train_accuracy: 6.852e-01, train_loss: 6.493e-01\n",
      "Epoch 3/100 | Batch 55/88 | train_accuracy: 6.886e-01, train_loss: 6.458e-01\n",
      "Epoch 3/100 | Batch 56/88 | train_accuracy: 6.853e-01, train_loss: 6.484e-01\n",
      "Epoch 3/100 | Batch 57/88 | train_accuracy: 6.842e-01, train_loss: 6.528e-01\n",
      "Epoch 3/100 | Batch 58/88 | train_accuracy: 6.853e-01, train_loss: 6.519e-01\n",
      "Epoch 3/100 | Batch 59/88 | train_accuracy: 6.854e-01, train_loss: 6.512e-01\n",
      "Epoch 3/100 | Batch 60/88 | train_accuracy: 6.854e-01, train_loss: 6.513e-01\n",
      "Epoch 3/100 | Batch 61/88 | train_accuracy: 6.895e-01, train_loss: 6.473e-01\n",
      "Epoch 3/100 | Batch 62/88 | train_accuracy: 6.915e-01, train_loss: 6.449e-01\n",
      "Epoch 3/100 | Batch 63/88 | train_accuracy: 6.915e-01, train_loss: 6.437e-01\n",
      "Epoch 3/100 | Batch 64/88 | train_accuracy: 6.865e-01, train_loss: 6.458e-01\n",
      "Epoch 3/100 | Batch 65/88 | train_accuracy: 6.865e-01, train_loss: 6.477e-01\n",
      "Epoch 3/100 | Batch 66/88 | train_accuracy: 6.837e-01, train_loss: 6.509e-01\n",
      "Epoch 3/100 | Batch 67/88 | train_accuracy: 6.838e-01, train_loss: 6.504e-01\n",
      "Epoch 3/100 | Batch 68/88 | train_accuracy: 6.811e-01, train_loss: 6.565e-01\n",
      "Epoch 3/100 | Batch 69/88 | train_accuracy: 6.812e-01, train_loss: 6.579e-01\n",
      "Epoch 3/100 | Batch 70/88 | train_accuracy: 6.830e-01, train_loss: 6.561e-01\n",
      "Epoch 3/100 | Batch 71/88 | train_accuracy: 6.822e-01, train_loss: 6.580e-01\n",
      "Epoch 3/100 | Batch 72/88 | train_accuracy: 6.840e-01, train_loss: 6.568e-01\n",
      "Epoch 3/100 | Batch 73/88 | train_accuracy: 6.858e-01, train_loss: 6.567e-01\n",
      "Epoch 3/100 | Batch 74/88 | train_accuracy: 6.875e-01, train_loss: 6.550e-01\n",
      "Epoch 3/100 | Batch 75/88 | train_accuracy: 6.858e-01, train_loss: 6.542e-01\n",
      "Epoch 3/100 | Batch 76/88 | train_accuracy: 6.859e-01, train_loss: 6.533e-01\n",
      "Epoch 3/100 | Batch 77/88 | train_accuracy: 6.859e-01, train_loss: 6.543e-01\n",
      "Epoch 3/100 | Batch 78/88 | train_accuracy: 6.859e-01, train_loss: 6.550e-01\n",
      "Epoch 3/100 | Batch 79/88 | train_accuracy: 6.867e-01, train_loss: 6.538e-01\n",
      "Epoch 3/100 | Batch 80/88 | train_accuracy: 6.883e-01, train_loss: 6.530e-01\n",
      "Epoch 3/100 | Batch 81/88 | train_accuracy: 6.875e-01, train_loss: 6.516e-01\n",
      "Epoch 3/100 | Batch 82/88 | train_accuracy: 6.867e-01, train_loss: 6.529e-01\n",
      "Epoch 3/100 | Batch 83/88 | train_accuracy: 6.875e-01, train_loss: 6.543e-01\n",
      "Epoch 3/100 | Batch 84/88 | train_accuracy: 6.882e-01, train_loss: 6.529e-01\n",
      "Epoch 3/100 | Batch 85/88 | train_accuracy: 6.882e-01, train_loss: 6.518e-01\n",
      "Epoch 3/100 | Batch 86/88 | train_accuracy: 6.882e-01, train_loss: 6.506e-01\n",
      "Epoch 3/100 | Batch 87/88 | train_accuracy: 6.897e-01, train_loss: 6.481e-01\n",
      "Epoch 3/100 | Batch 88/88 | train_accuracy: 6.914e-01, train_loss: 6.441e-01\n",
      "Epoch 3/100 | val_accuracy: 6.750e-01, val_loss: 6.837e-01\n",
      "====================\n",
      "Epoch 4/100 | Batch 1/88 | train_accuracy: 7.500e-01, train_loss: 6.584e-01\n",
      "Epoch 4/100 | Batch 2/88 | train_accuracy: 7.812e-01, train_loss: 6.183e-01\n",
      "Epoch 4/100 | Batch 3/88 | train_accuracy: 7.917e-01, train_loss: 5.594e-01\n",
      "Epoch 4/100 | Batch 4/88 | train_accuracy: 7.812e-01, train_loss: 5.574e-01\n",
      "Epoch 4/100 | Batch 5/88 | train_accuracy: 7.500e-01, train_loss: 5.760e-01\n",
      "Epoch 4/100 | Batch 6/88 | train_accuracy: 7.604e-01, train_loss: 5.605e-01\n",
      "Epoch 4/100 | Batch 7/88 | train_accuracy: 7.679e-01, train_loss: 5.227e-01\n",
      "Epoch 4/100 | Batch 8/88 | train_accuracy: 7.500e-01, train_loss: 5.912e-01\n",
      "Epoch 4/100 | Batch 9/88 | train_accuracy: 7.639e-01, train_loss: 5.733e-01\n",
      "Epoch 4/100 | Batch 10/88 | train_accuracy: 7.688e-01, train_loss: 5.582e-01\n",
      "Epoch 4/100 | Batch 11/88 | train_accuracy: 7.614e-01, train_loss: 5.523e-01\n",
      "Epoch 4/100 | Batch 12/88 | train_accuracy: 7.656e-01, train_loss: 5.503e-01\n",
      "Epoch 4/100 | Batch 13/88 | train_accuracy: 7.644e-01, train_loss: 5.492e-01\n",
      "Epoch 4/100 | Batch 14/88 | train_accuracy: 7.723e-01, train_loss: 5.325e-01\n",
      "Epoch 4/100 | Batch 15/88 | train_accuracy: 7.833e-01, train_loss: 5.230e-01\n",
      "Epoch 4/100 | Batch 16/88 | train_accuracy: 7.930e-01, train_loss: 5.086e-01\n",
      "Epoch 4/100 | Batch 17/88 | train_accuracy: 7.904e-01, train_loss: 5.053e-01\n",
      "Epoch 4/100 | Batch 18/88 | train_accuracy: 7.917e-01, train_loss: 5.039e-01\n",
      "Epoch 4/100 | Batch 19/88 | train_accuracy: 7.895e-01, train_loss: 5.029e-01\n",
      "Epoch 4/100 | Batch 20/88 | train_accuracy: 7.937e-01, train_loss: 4.969e-01\n",
      "Epoch 4/100 | Batch 21/88 | train_accuracy: 7.887e-01, train_loss: 4.952e-01\n",
      "Epoch 4/100 | Batch 22/88 | train_accuracy: 7.898e-01, train_loss: 4.919e-01\n",
      "Epoch 4/100 | Batch 23/88 | train_accuracy: 7.935e-01, train_loss: 4.845e-01\n",
      "Epoch 4/100 | Batch 24/88 | train_accuracy: 7.865e-01, train_loss: 4.890e-01\n",
      "Epoch 4/100 | Batch 25/88 | train_accuracy: 7.750e-01, train_loss: 5.183e-01\n",
      "Epoch 4/100 | Batch 26/88 | train_accuracy: 7.668e-01, train_loss: 5.217e-01\n",
      "Epoch 4/100 | Batch 27/88 | train_accuracy: 7.639e-01, train_loss: 5.195e-01\n",
      "Epoch 4/100 | Batch 28/88 | train_accuracy: 7.589e-01, train_loss: 5.245e-01\n",
      "Epoch 4/100 | Batch 29/88 | train_accuracy: 7.651e-01, train_loss: 5.175e-01\n",
      "Epoch 4/100 | Batch 30/88 | train_accuracy: 7.646e-01, train_loss: 5.214e-01\n",
      "Epoch 4/100 | Batch 31/88 | train_accuracy: 7.601e-01, train_loss: 5.233e-01\n",
      "Epoch 4/100 | Batch 32/88 | train_accuracy: 7.637e-01, train_loss: 5.169e-01\n",
      "Epoch 4/100 | Batch 33/88 | train_accuracy: 7.633e-01, train_loss: 5.170e-01\n",
      "Epoch 4/100 | Batch 34/88 | train_accuracy: 7.629e-01, train_loss: 5.155e-01\n",
      "Epoch 4/100 | Batch 35/88 | train_accuracy: 7.625e-01, train_loss: 5.155e-01\n",
      "Epoch 4/100 | Batch 36/88 | train_accuracy: 7.656e-01, train_loss: 5.123e-01\n",
      "Epoch 4/100 | Batch 37/88 | train_accuracy: 7.652e-01, train_loss: 5.115e-01\n",
      "Epoch 4/100 | Batch 38/88 | train_accuracy: 7.648e-01, train_loss: 5.114e-01\n",
      "Epoch 4/100 | Batch 39/88 | train_accuracy: 7.612e-01, train_loss: 5.186e-01\n",
      "Epoch 4/100 | Batch 40/88 | train_accuracy: 7.625e-01, train_loss: 5.140e-01\n",
      "Epoch 4/100 | Batch 41/88 | train_accuracy: 7.591e-01, train_loss: 5.211e-01\n",
      "Epoch 4/100 | Batch 42/88 | train_accuracy: 7.589e-01, train_loss: 5.211e-01\n",
      "Epoch 4/100 | Batch 43/88 | train_accuracy: 7.515e-01, train_loss: 5.318e-01\n",
      "Epoch 4/100 | Batch 44/88 | train_accuracy: 7.457e-01, train_loss: 5.404e-01\n",
      "Epoch 4/100 | Batch 45/88 | train_accuracy: 7.458e-01, train_loss: 5.436e-01\n",
      "Epoch 4/100 | Batch 46/88 | train_accuracy: 7.459e-01, train_loss: 5.447e-01\n",
      "Epoch 4/100 | Batch 47/88 | train_accuracy: 7.473e-01, train_loss: 5.433e-01\n",
      "Epoch 4/100 | Batch 48/88 | train_accuracy: 7.461e-01, train_loss: 5.452e-01\n",
      "Epoch 4/100 | Batch 49/88 | train_accuracy: 7.462e-01, train_loss: 5.477e-01\n",
      "Epoch 4/100 | Batch 50/88 | train_accuracy: 7.438e-01, train_loss: 5.504e-01\n",
      "Epoch 4/100 | Batch 51/88 | train_accuracy: 7.451e-01, train_loss: 5.479e-01\n",
      "Epoch 4/100 | Batch 52/88 | train_accuracy: 7.452e-01, train_loss: 5.471e-01\n",
      "Epoch 4/100 | Batch 53/88 | train_accuracy: 7.465e-01, train_loss: 5.465e-01\n",
      "Epoch 4/100 | Batch 54/88 | train_accuracy: 7.454e-01, train_loss: 5.476e-01\n",
      "Epoch 4/100 | Batch 55/88 | train_accuracy: 7.455e-01, train_loss: 5.477e-01\n",
      "Epoch 4/100 | Batch 56/88 | train_accuracy: 7.467e-01, train_loss: 5.475e-01\n",
      "Epoch 4/100 | Batch 57/88 | train_accuracy: 7.456e-01, train_loss: 5.479e-01\n",
      "Epoch 4/100 | Batch 58/88 | train_accuracy: 7.468e-01, train_loss: 5.469e-01\n",
      "Epoch 4/100 | Batch 59/88 | train_accuracy: 7.468e-01, train_loss: 5.448e-01\n",
      "Epoch 4/100 | Batch 60/88 | train_accuracy: 7.438e-01, train_loss: 5.489e-01\n",
      "Epoch 4/100 | Batch 61/88 | train_accuracy: 7.418e-01, train_loss: 5.502e-01\n",
      "Epoch 4/100 | Batch 62/88 | train_accuracy: 7.440e-01, train_loss: 5.471e-01\n",
      "Epoch 4/100 | Batch 63/88 | train_accuracy: 7.440e-01, train_loss: 5.468e-01\n",
      "Epoch 4/100 | Batch 64/88 | train_accuracy: 7.432e-01, train_loss: 5.459e-01\n",
      "Epoch 4/100 | Batch 65/88 | train_accuracy: 7.394e-01, train_loss: 5.601e-01\n",
      "Epoch 4/100 | Batch 66/88 | train_accuracy: 7.415e-01, train_loss: 5.608e-01\n",
      "Epoch 4/100 | Batch 67/88 | train_accuracy: 7.425e-01, train_loss: 5.600e-01\n",
      "Epoch 4/100 | Batch 68/88 | train_accuracy: 7.426e-01, train_loss: 5.584e-01\n",
      "Epoch 4/100 | Batch 69/88 | train_accuracy: 7.418e-01, train_loss: 5.597e-01\n",
      "Epoch 4/100 | Batch 70/88 | train_accuracy: 7.438e-01, train_loss: 5.592e-01\n",
      "Epoch 4/100 | Batch 71/88 | train_accuracy: 7.430e-01, train_loss: 5.621e-01\n",
      "Epoch 4/100 | Batch 72/88 | train_accuracy: 7.413e-01, train_loss: 5.638e-01\n",
      "Epoch 4/100 | Batch 73/88 | train_accuracy: 7.389e-01, train_loss: 5.651e-01\n",
      "Epoch 4/100 | Batch 74/88 | train_accuracy: 7.365e-01, train_loss: 5.660e-01\n",
      "Epoch 4/100 | Batch 75/88 | train_accuracy: 7.383e-01, train_loss: 5.635e-01\n",
      "Epoch 4/100 | Batch 76/88 | train_accuracy: 7.385e-01, train_loss: 5.628e-01\n",
      "Epoch 4/100 | Batch 77/88 | train_accuracy: 7.330e-01, train_loss: 5.692e-01\n",
      "Epoch 4/100 | Batch 78/88 | train_accuracy: 7.348e-01, train_loss: 5.669e-01\n",
      "Epoch 4/100 | Batch 79/88 | train_accuracy: 7.342e-01, train_loss: 5.685e-01\n",
      "Epoch 4/100 | Batch 80/88 | train_accuracy: 7.344e-01, train_loss: 5.683e-01\n",
      "Epoch 4/100 | Batch 81/88 | train_accuracy: 7.346e-01, train_loss: 5.678e-01\n",
      "Epoch 4/100 | Batch 82/88 | train_accuracy: 7.348e-01, train_loss: 5.678e-01\n",
      "Epoch 4/100 | Batch 83/88 | train_accuracy: 7.334e-01, train_loss: 5.689e-01\n",
      "Epoch 4/100 | Batch 84/88 | train_accuracy: 7.344e-01, train_loss: 5.683e-01\n",
      "Epoch 4/100 | Batch 85/88 | train_accuracy: 7.338e-01, train_loss: 5.690e-01\n",
      "Epoch 4/100 | Batch 86/88 | train_accuracy: 7.355e-01, train_loss: 5.662e-01\n",
      "Epoch 4/100 | Batch 87/88 | train_accuracy: 7.364e-01, train_loss: 5.652e-01\n",
      "Epoch 4/100 | Batch 88/88 | train_accuracy: 7.364e-01, train_loss: 5.641e-01\n",
      "Epoch 4/100 | val_accuracy: 7.000e-01, val_loss: 6.172e-01\n",
      "====================\n",
      "Epoch 5/100 | Batch 1/88 | train_accuracy: 8.750e-01, train_loss: 4.876e-01\n",
      "Epoch 5/100 | Batch 2/88 | train_accuracy: 9.062e-01, train_loss: 3.863e-01\n",
      "Epoch 5/100 | Batch 3/88 | train_accuracy: 8.750e-01, train_loss: 4.577e-01\n",
      "Epoch 5/100 | Batch 4/88 | train_accuracy: 8.438e-01, train_loss: 5.138e-01\n",
      "Epoch 5/100 | Batch 5/88 | train_accuracy: 8.625e-01, train_loss: 4.997e-01\n",
      "Epoch 5/100 | Batch 6/88 | train_accuracy: 8.542e-01, train_loss: 4.960e-01\n",
      "Epoch 5/100 | Batch 7/88 | train_accuracy: 8.304e-01, train_loss: 4.920e-01\n",
      "Epoch 5/100 | Batch 8/88 | train_accuracy: 8.516e-01, train_loss: 4.604e-01\n",
      "Epoch 5/100 | Batch 9/88 | train_accuracy: 8.611e-01, train_loss: 4.415e-01\n",
      "Epoch 5/100 | Batch 10/88 | train_accuracy: 8.500e-01, train_loss: 4.663e-01\n",
      "Epoch 5/100 | Batch 11/88 | train_accuracy: 8.409e-01, train_loss: 4.694e-01\n",
      "Epoch 5/100 | Batch 12/88 | train_accuracy: 8.385e-01, train_loss: 4.594e-01\n",
      "Epoch 5/100 | Batch 13/88 | train_accuracy: 8.317e-01, train_loss: 4.643e-01\n",
      "Epoch 5/100 | Batch 14/88 | train_accuracy: 8.170e-01, train_loss: 4.768e-01\n",
      "Epoch 5/100 | Batch 15/88 | train_accuracy: 8.250e-01, train_loss: 4.698e-01\n",
      "Epoch 5/100 | Batch 16/88 | train_accuracy: 8.164e-01, train_loss: 4.740e-01\n",
      "Epoch 5/100 | Batch 17/88 | train_accuracy: 8.125e-01, train_loss: 4.752e-01\n",
      "Epoch 5/100 | Batch 18/88 | train_accuracy: 8.090e-01, train_loss: 4.750e-01\n",
      "Epoch 5/100 | Batch 19/88 | train_accuracy: 7.961e-01, train_loss: 4.953e-01\n",
      "Epoch 5/100 | Batch 20/88 | train_accuracy: 7.906e-01, train_loss: 4.972e-01\n",
      "Epoch 5/100 | Batch 21/88 | train_accuracy: 7.976e-01, train_loss: 5.116e-01\n",
      "Epoch 5/100 | Batch 22/88 | train_accuracy: 7.983e-01, train_loss: 5.055e-01\n",
      "Epoch 5/100 | Batch 23/88 | train_accuracy: 8.016e-01, train_loss: 4.992e-01\n",
      "Epoch 5/100 | Batch 24/88 | train_accuracy: 8.047e-01, train_loss: 4.945e-01\n",
      "Epoch 5/100 | Batch 25/88 | train_accuracy: 8.000e-01, train_loss: 4.932e-01\n",
      "Epoch 5/100 | Batch 26/88 | train_accuracy: 7.981e-01, train_loss: 4.924e-01\n",
      "Epoch 5/100 | Batch 27/88 | train_accuracy: 8.009e-01, train_loss: 4.920e-01\n",
      "Epoch 5/100 | Batch 28/88 | train_accuracy: 8.058e-01, train_loss: 4.890e-01\n",
      "Epoch 5/100 | Batch 29/88 | train_accuracy: 8.017e-01, train_loss: 4.955e-01\n",
      "Epoch 5/100 | Batch 30/88 | train_accuracy: 8.042e-01, train_loss: 4.920e-01\n",
      "Epoch 5/100 | Batch 31/88 | train_accuracy: 8.044e-01, train_loss: 4.964e-01\n",
      "Epoch 5/100 | Batch 32/88 | train_accuracy: 8.027e-01, train_loss: 4.997e-01\n",
      "Epoch 5/100 | Batch 33/88 | train_accuracy: 8.030e-01, train_loss: 5.084e-01\n",
      "Epoch 5/100 | Batch 34/88 | train_accuracy: 8.015e-01, train_loss: 5.073e-01\n",
      "Epoch 5/100 | Batch 35/88 | train_accuracy: 7.946e-01, train_loss: 5.105e-01\n",
      "Epoch 5/100 | Batch 36/88 | train_accuracy: 7.951e-01, train_loss: 5.127e-01\n",
      "Epoch 5/100 | Batch 37/88 | train_accuracy: 7.905e-01, train_loss: 5.139e-01\n",
      "Epoch 5/100 | Batch 38/88 | train_accuracy: 7.911e-01, train_loss: 5.118e-01\n",
      "Epoch 5/100 | Batch 39/88 | train_accuracy: 7.933e-01, train_loss: 5.101e-01\n",
      "Epoch 5/100 | Batch 40/88 | train_accuracy: 7.937e-01, train_loss: 5.078e-01\n",
      "Epoch 5/100 | Batch 41/88 | train_accuracy: 7.927e-01, train_loss: 5.066e-01\n",
      "Epoch 5/100 | Batch 42/88 | train_accuracy: 7.887e-01, train_loss: 5.088e-01\n",
      "Epoch 5/100 | Batch 43/88 | train_accuracy: 7.892e-01, train_loss: 5.066e-01\n",
      "Epoch 5/100 | Batch 44/88 | train_accuracy: 7.869e-01, train_loss: 5.063e-01\n",
      "Epoch 5/100 | Batch 45/88 | train_accuracy: 7.819e-01, train_loss: 5.093e-01\n",
      "Epoch 5/100 | Batch 46/88 | train_accuracy: 7.840e-01, train_loss: 5.059e-01\n",
      "Epoch 5/100 | Batch 47/88 | train_accuracy: 7.846e-01, train_loss: 5.049e-01\n",
      "Epoch 5/100 | Batch 48/88 | train_accuracy: 7.839e-01, train_loss: 5.034e-01\n",
      "Epoch 5/100 | Batch 49/88 | train_accuracy: 7.844e-01, train_loss: 5.019e-01\n",
      "Epoch 5/100 | Batch 50/88 | train_accuracy: 7.837e-01, train_loss: 5.021e-01\n",
      "Epoch 5/100 | Batch 51/88 | train_accuracy: 7.880e-01, train_loss: 4.992e-01\n",
      "Epoch 5/100 | Batch 52/88 | train_accuracy: 7.897e-01, train_loss: 4.973e-01\n",
      "Epoch 5/100 | Batch 53/88 | train_accuracy: 7.901e-01, train_loss: 4.972e-01\n",
      "Epoch 5/100 | Batch 54/88 | train_accuracy: 7.905e-01, train_loss: 4.960e-01\n",
      "Epoch 5/100 | Batch 55/88 | train_accuracy: 7.875e-01, train_loss: 4.982e-01\n",
      "Epoch 5/100 | Batch 56/88 | train_accuracy: 7.879e-01, train_loss: 4.959e-01\n",
      "Epoch 5/100 | Batch 57/88 | train_accuracy: 7.906e-01, train_loss: 4.928e-01\n",
      "Epoch 5/100 | Batch 58/88 | train_accuracy: 7.899e-01, train_loss: 5.027e-01\n",
      "Epoch 5/100 | Batch 59/88 | train_accuracy: 7.903e-01, train_loss: 5.015e-01\n",
      "Epoch 5/100 | Batch 60/88 | train_accuracy: 7.885e-01, train_loss: 5.035e-01\n",
      "Epoch 5/100 | Batch 61/88 | train_accuracy: 7.900e-01, train_loss: 5.002e-01\n",
      "Epoch 5/100 | Batch 62/88 | train_accuracy: 7.913e-01, train_loss: 4.960e-01\n",
      "Epoch 5/100 | Batch 63/88 | train_accuracy: 7.907e-01, train_loss: 4.954e-01\n",
      "Epoch 5/100 | Batch 64/88 | train_accuracy: 7.900e-01, train_loss: 4.974e-01\n",
      "Epoch 5/100 | Batch 65/88 | train_accuracy: 7.904e-01, train_loss: 4.958e-01\n",
      "Epoch 5/100 | Batch 66/88 | train_accuracy: 7.888e-01, train_loss: 4.974e-01\n",
      "Epoch 5/100 | Batch 67/88 | train_accuracy: 7.873e-01, train_loss: 4.984e-01\n",
      "Epoch 5/100 | Batch 68/88 | train_accuracy: 7.895e-01, train_loss: 4.950e-01\n",
      "Epoch 5/100 | Batch 69/88 | train_accuracy: 7.880e-01, train_loss: 4.950e-01\n",
      "Epoch 5/100 | Batch 70/88 | train_accuracy: 7.857e-01, train_loss: 4.994e-01\n",
      "Epoch 5/100 | Batch 71/88 | train_accuracy: 7.861e-01, train_loss: 4.976e-01\n",
      "Epoch 5/100 | Batch 72/88 | train_accuracy: 7.830e-01, train_loss: 5.017e-01\n",
      "Epoch 5/100 | Batch 73/88 | train_accuracy: 7.834e-01, train_loss: 5.015e-01\n",
      "Epoch 5/100 | Batch 74/88 | train_accuracy: 7.829e-01, train_loss: 5.017e-01\n",
      "Epoch 5/100 | Batch 75/88 | train_accuracy: 7.833e-01, train_loss: 5.010e-01\n",
      "Epoch 5/100 | Batch 76/88 | train_accuracy: 7.837e-01, train_loss: 5.003e-01\n",
      "Epoch 5/100 | Batch 77/88 | train_accuracy: 7.857e-01, train_loss: 4.997e-01\n",
      "Epoch 5/100 | Batch 78/88 | train_accuracy: 7.829e-01, train_loss: 5.018e-01\n",
      "Epoch 5/100 | Batch 79/88 | train_accuracy: 7.832e-01, train_loss: 5.010e-01\n",
      "Epoch 5/100 | Batch 80/88 | train_accuracy: 7.789e-01, train_loss: 5.052e-01\n",
      "Epoch 5/100 | Batch 81/88 | train_accuracy: 7.770e-01, train_loss: 5.075e-01\n",
      "Epoch 5/100 | Batch 82/88 | train_accuracy: 7.767e-01, train_loss: 5.069e-01\n",
      "Epoch 5/100 | Batch 83/88 | train_accuracy: 7.771e-01, train_loss: 5.077e-01\n",
      "Epoch 5/100 | Batch 84/88 | train_accuracy: 7.768e-01, train_loss: 5.083e-01\n",
      "Epoch 5/100 | Batch 85/88 | train_accuracy: 7.779e-01, train_loss: 5.053e-01\n",
      "Epoch 5/100 | Batch 86/88 | train_accuracy: 7.776e-01, train_loss: 5.062e-01\n",
      "Epoch 5/100 | Batch 87/88 | train_accuracy: 7.773e-01, train_loss: 5.041e-01\n",
      "Epoch 5/100 | Batch 88/88 | train_accuracy: 7.779e-01, train_loss: 5.035e-01\n",
      "Epoch 5/100 | val_accuracy: 6.800e-01, val_loss: 6.455e-01\n",
      "====================\n",
      "Epoch 6/100 | Batch 1/88 | train_accuracy: 8.125e-01, train_loss: 4.786e-01\n",
      "Epoch 6/100 | Batch 2/88 | train_accuracy: 8.438e-01, train_loss: 4.595e-01\n",
      "Epoch 6/100 | Batch 3/88 | train_accuracy: 7.917e-01, train_loss: 5.089e-01\n",
      "Epoch 6/100 | Batch 4/88 | train_accuracy: 8.125e-01, train_loss: 4.547e-01\n",
      "Epoch 6/100 | Batch 5/88 | train_accuracy: 7.625e-01, train_loss: 4.894e-01\n",
      "Epoch 6/100 | Batch 6/88 | train_accuracy: 7.708e-01, train_loss: 4.810e-01\n",
      "Epoch 6/100 | Batch 7/88 | train_accuracy: 7.589e-01, train_loss: 4.770e-01\n",
      "Epoch 6/100 | Batch 8/88 | train_accuracy: 7.500e-01, train_loss: 4.735e-01\n",
      "Epoch 6/100 | Batch 9/88 | train_accuracy: 7.639e-01, train_loss: 4.534e-01\n",
      "Epoch 6/100 | Batch 10/88 | train_accuracy: 7.750e-01, train_loss: 4.794e-01\n",
      "Epoch 6/100 | Batch 11/88 | train_accuracy: 7.841e-01, train_loss: 4.785e-01\n",
      "Epoch 6/100 | Batch 12/88 | train_accuracy: 7.917e-01, train_loss: 4.662e-01\n",
      "Epoch 6/100 | Batch 13/88 | train_accuracy: 7.885e-01, train_loss: 4.670e-01\n",
      "Epoch 6/100 | Batch 14/88 | train_accuracy: 7.946e-01, train_loss: 4.578e-01\n",
      "Epoch 6/100 | Batch 15/88 | train_accuracy: 8.000e-01, train_loss: 4.411e-01\n",
      "Epoch 6/100 | Batch 16/88 | train_accuracy: 7.969e-01, train_loss: 4.481e-01\n",
      "Epoch 6/100 | Batch 17/88 | train_accuracy: 8.051e-01, train_loss: 4.399e-01\n",
      "Epoch 6/100 | Batch 18/88 | train_accuracy: 8.056e-01, train_loss: 4.465e-01\n",
      "Epoch 6/100 | Batch 19/88 | train_accuracy: 8.059e-01, train_loss: 4.459e-01\n",
      "Epoch 6/100 | Batch 20/88 | train_accuracy: 8.125e-01, train_loss: 4.371e-01\n",
      "Epoch 6/100 | Batch 21/88 | train_accuracy: 8.095e-01, train_loss: 4.419e-01\n",
      "Epoch 6/100 | Batch 22/88 | train_accuracy: 8.182e-01, train_loss: 4.297e-01\n",
      "Epoch 6/100 | Batch 23/88 | train_accuracy: 8.152e-01, train_loss: 4.369e-01\n",
      "Epoch 6/100 | Batch 24/88 | train_accuracy: 8.177e-01, train_loss: 4.297e-01\n",
      "Epoch 6/100 | Batch 25/88 | train_accuracy: 8.175e-01, train_loss: 4.319e-01\n",
      "Epoch 6/100 | Batch 26/88 | train_accuracy: 8.101e-01, train_loss: 4.414e-01\n",
      "Epoch 6/100 | Batch 27/88 | train_accuracy: 8.102e-01, train_loss: 4.390e-01\n",
      "Epoch 6/100 | Batch 28/88 | train_accuracy: 8.125e-01, train_loss: 4.394e-01\n",
      "Epoch 6/100 | Batch 29/88 | train_accuracy: 8.039e-01, train_loss: 4.539e-01\n",
      "Epoch 6/100 | Batch 30/88 | train_accuracy: 8.000e-01, train_loss: 4.553e-01\n",
      "Epoch 6/100 | Batch 31/88 | train_accuracy: 8.004e-01, train_loss: 4.538e-01\n",
      "Epoch 6/100 | Batch 32/88 | train_accuracy: 7.988e-01, train_loss: 4.541e-01\n",
      "Epoch 6/100 | Batch 33/88 | train_accuracy: 7.992e-01, train_loss: 4.560e-01\n",
      "Epoch 6/100 | Batch 34/88 | train_accuracy: 8.015e-01, train_loss: 4.575e-01\n",
      "Epoch 6/100 | Batch 35/88 | train_accuracy: 8.018e-01, train_loss: 4.564e-01\n",
      "Epoch 6/100 | Batch 36/88 | train_accuracy: 8.038e-01, train_loss: 4.566e-01\n",
      "Epoch 6/100 | Batch 37/88 | train_accuracy: 8.057e-01, train_loss: 4.538e-01\n",
      "Epoch 6/100 | Batch 38/88 | train_accuracy: 8.059e-01, train_loss: 4.505e-01\n",
      "Epoch 6/100 | Batch 39/88 | train_accuracy: 8.077e-01, train_loss: 4.475e-01\n",
      "Epoch 6/100 | Batch 40/88 | train_accuracy: 8.094e-01, train_loss: 4.449e-01\n",
      "Epoch 6/100 | Batch 41/88 | train_accuracy: 8.095e-01, train_loss: 4.413e-01\n",
      "Epoch 6/100 | Batch 42/88 | train_accuracy: 8.125e-01, train_loss: 4.367e-01\n",
      "Epoch 6/100 | Batch 43/88 | train_accuracy: 8.140e-01, train_loss: 4.334e-01\n",
      "Epoch 6/100 | Batch 44/88 | train_accuracy: 8.125e-01, train_loss: 4.335e-01\n",
      "Epoch 6/100 | Batch 45/88 | train_accuracy: 8.139e-01, train_loss: 4.328e-01\n",
      "Epoch 6/100 | Batch 46/88 | train_accuracy: 8.166e-01, train_loss: 4.287e-01\n",
      "Epoch 6/100 | Batch 47/88 | train_accuracy: 8.165e-01, train_loss: 4.289e-01\n",
      "Epoch 6/100 | Batch 48/88 | train_accuracy: 8.203e-01, train_loss: 4.227e-01\n",
      "Epoch 6/100 | Batch 49/88 | train_accuracy: 8.163e-01, train_loss: 4.277e-01\n",
      "Epoch 6/100 | Batch 50/88 | train_accuracy: 8.163e-01, train_loss: 4.269e-01\n",
      "Epoch 6/100 | Batch 51/88 | train_accuracy: 8.186e-01, train_loss: 4.229e-01\n",
      "Epoch 6/100 | Batch 52/88 | train_accuracy: 8.197e-01, train_loss: 4.192e-01\n",
      "Epoch 6/100 | Batch 53/88 | train_accuracy: 8.208e-01, train_loss: 4.162e-01\n",
      "Epoch 6/100 | Batch 54/88 | train_accuracy: 8.171e-01, train_loss: 4.274e-01\n",
      "Epoch 6/100 | Batch 55/88 | train_accuracy: 8.182e-01, train_loss: 4.248e-01\n",
      "Epoch 6/100 | Batch 56/88 | train_accuracy: 8.203e-01, train_loss: 4.227e-01\n",
      "Epoch 6/100 | Batch 57/88 | train_accuracy: 8.202e-01, train_loss: 4.207e-01\n",
      "Epoch 6/100 | Batch 58/88 | train_accuracy: 8.179e-01, train_loss: 4.250e-01\n",
      "Epoch 6/100 | Batch 59/88 | train_accuracy: 8.136e-01, train_loss: 4.319e-01\n",
      "Epoch 6/100 | Batch 60/88 | train_accuracy: 8.135e-01, train_loss: 4.350e-01\n",
      "Epoch 6/100 | Batch 61/88 | train_accuracy: 8.145e-01, train_loss: 4.325e-01\n",
      "Epoch 6/100 | Batch 62/88 | train_accuracy: 8.135e-01, train_loss: 4.317e-01\n",
      "Epoch 6/100 | Batch 63/88 | train_accuracy: 8.135e-01, train_loss: 4.329e-01\n",
      "Epoch 6/100 | Batch 64/88 | train_accuracy: 8.135e-01, train_loss: 4.326e-01\n",
      "Epoch 6/100 | Batch 65/88 | train_accuracy: 8.154e-01, train_loss: 4.300e-01\n",
      "Epoch 6/100 | Batch 66/88 | train_accuracy: 8.125e-01, train_loss: 4.340e-01\n",
      "Epoch 6/100 | Batch 67/88 | train_accuracy: 8.134e-01, train_loss: 4.343e-01\n",
      "Epoch 6/100 | Batch 68/88 | train_accuracy: 8.125e-01, train_loss: 4.345e-01\n",
      "Epoch 6/100 | Batch 69/88 | train_accuracy: 8.143e-01, train_loss: 4.340e-01\n",
      "Epoch 6/100 | Batch 70/88 | train_accuracy: 8.116e-01, train_loss: 4.367e-01\n",
      "Epoch 6/100 | Batch 71/88 | train_accuracy: 8.099e-01, train_loss: 4.393e-01\n",
      "Epoch 6/100 | Batch 72/88 | train_accuracy: 8.082e-01, train_loss: 4.399e-01\n",
      "Epoch 6/100 | Batch 73/88 | train_accuracy: 8.065e-01, train_loss: 4.416e-01\n",
      "Epoch 6/100 | Batch 74/88 | train_accuracy: 8.057e-01, train_loss: 4.421e-01\n",
      "Epoch 6/100 | Batch 75/88 | train_accuracy: 8.075e-01, train_loss: 4.410e-01\n",
      "Epoch 6/100 | Batch 76/88 | train_accuracy: 8.051e-01, train_loss: 4.463e-01\n",
      "Epoch 6/100 | Batch 77/88 | train_accuracy: 7.995e-01, train_loss: 4.505e-01\n",
      "Epoch 6/100 | Batch 78/88 | train_accuracy: 7.989e-01, train_loss: 4.512e-01\n",
      "Epoch 6/100 | Batch 79/88 | train_accuracy: 8.014e-01, train_loss: 4.492e-01\n",
      "Epoch 6/100 | Batch 80/88 | train_accuracy: 8.031e-01, train_loss: 4.471e-01\n",
      "Epoch 6/100 | Batch 81/88 | train_accuracy: 8.025e-01, train_loss: 4.476e-01\n",
      "Epoch 6/100 | Batch 82/88 | train_accuracy: 8.026e-01, train_loss: 4.501e-01\n",
      "Epoch 6/100 | Batch 83/88 | train_accuracy: 7.997e-01, train_loss: 4.522e-01\n",
      "Epoch 6/100 | Batch 84/88 | train_accuracy: 7.991e-01, train_loss: 4.531e-01\n",
      "Epoch 6/100 | Batch 85/88 | train_accuracy: 8.015e-01, train_loss: 4.500e-01\n",
      "Epoch 6/100 | Batch 86/88 | train_accuracy: 8.016e-01, train_loss: 4.495e-01\n",
      "Epoch 6/100 | Batch 87/88 | train_accuracy: 8.024e-01, train_loss: 4.481e-01\n",
      "Epoch 6/100 | Batch 88/88 | train_accuracy: 8.014e-01, train_loss: 4.511e-01\n",
      "Epoch 6/100 | val_accuracy: 6.950e-01, val_loss: 6.148e-01\n",
      "====================\n",
      "Epoch 7/100 | Batch 1/88 | train_accuracy: 7.500e-01, train_loss: 6.050e-01\n",
      "Epoch 7/100 | Batch 2/88 | train_accuracy: 8.438e-01, train_loss: 4.048e-01\n",
      "Epoch 7/100 | Batch 3/88 | train_accuracy: 8.333e-01, train_loss: 3.749e-01\n",
      "Epoch 7/100 | Batch 4/88 | train_accuracy: 8.438e-01, train_loss: 3.975e-01\n",
      "Epoch 7/100 | Batch 5/88 | train_accuracy: 8.375e-01, train_loss: 3.893e-01\n",
      "Epoch 7/100 | Batch 6/88 | train_accuracy: 8.438e-01, train_loss: 3.834e-01\n",
      "Epoch 7/100 | Batch 7/88 | train_accuracy: 8.482e-01, train_loss: 3.662e-01\n",
      "Epoch 7/100 | Batch 8/88 | train_accuracy: 8.516e-01, train_loss: 3.537e-01\n",
      "Epoch 7/100 | Batch 9/88 | train_accuracy: 8.542e-01, train_loss: 3.446e-01\n",
      "Epoch 7/100 | Batch 10/88 | train_accuracy: 8.438e-01, train_loss: 3.517e-01\n",
      "Epoch 7/100 | Batch 11/88 | train_accuracy: 8.295e-01, train_loss: 3.741e-01\n",
      "Epoch 7/100 | Batch 12/88 | train_accuracy: 8.333e-01, train_loss: 3.648e-01\n",
      "Epoch 7/100 | Batch 13/88 | train_accuracy: 8.269e-01, train_loss: 3.755e-01\n",
      "Epoch 7/100 | Batch 14/88 | train_accuracy: 8.214e-01, train_loss: 3.774e-01\n",
      "Epoch 7/100 | Batch 15/88 | train_accuracy: 8.250e-01, train_loss: 3.762e-01\n",
      "Epoch 7/100 | Batch 16/88 | train_accuracy: 8.320e-01, train_loss: 3.658e-01\n",
      "Epoch 7/100 | Batch 17/88 | train_accuracy: 8.272e-01, train_loss: 3.715e-01\n",
      "Epoch 7/100 | Batch 18/88 | train_accuracy: 8.333e-01, train_loss: 3.646e-01\n",
      "Epoch 7/100 | Batch 19/88 | train_accuracy: 8.289e-01, train_loss: 3.673e-01\n",
      "Epoch 7/100 | Batch 20/88 | train_accuracy: 8.375e-01, train_loss: 3.580e-01\n",
      "Epoch 7/100 | Batch 21/88 | train_accuracy: 8.423e-01, train_loss: 3.542e-01\n",
      "Epoch 7/100 | Batch 22/88 | train_accuracy: 8.438e-01, train_loss: 3.491e-01\n",
      "Epoch 7/100 | Batch 23/88 | train_accuracy: 8.505e-01, train_loss: 3.388e-01\n",
      "Epoch 7/100 | Batch 24/88 | train_accuracy: 8.490e-01, train_loss: 3.458e-01\n",
      "Epoch 7/100 | Batch 25/88 | train_accuracy: 8.475e-01, train_loss: 3.439e-01\n",
      "Epoch 7/100 | Batch 26/88 | train_accuracy: 8.486e-01, train_loss: 3.432e-01\n",
      "Epoch 7/100 | Batch 27/88 | train_accuracy: 8.472e-01, train_loss: 3.465e-01\n",
      "Epoch 7/100 | Batch 28/88 | train_accuracy: 8.438e-01, train_loss: 3.573e-01\n",
      "Epoch 7/100 | Batch 29/88 | train_accuracy: 8.362e-01, train_loss: 3.727e-01\n",
      "Epoch 7/100 | Batch 30/88 | train_accuracy: 8.396e-01, train_loss: 3.717e-01\n",
      "Epoch 7/100 | Batch 31/88 | train_accuracy: 8.407e-01, train_loss: 3.702e-01\n",
      "Epoch 7/100 | Batch 32/88 | train_accuracy: 8.418e-01, train_loss: 3.664e-01\n",
      "Epoch 7/100 | Batch 33/88 | train_accuracy: 8.447e-01, train_loss: 3.615e-01\n",
      "Epoch 7/100 | Batch 34/88 | train_accuracy: 8.438e-01, train_loss: 3.623e-01\n",
      "Epoch 7/100 | Batch 35/88 | train_accuracy: 8.464e-01, train_loss: 3.559e-01\n",
      "Epoch 7/100 | Batch 36/88 | train_accuracy: 8.472e-01, train_loss: 3.551e-01\n",
      "Epoch 7/100 | Batch 37/88 | train_accuracy: 8.480e-01, train_loss: 3.519e-01\n",
      "Epoch 7/100 | Batch 38/88 | train_accuracy: 8.520e-01, train_loss: 3.465e-01\n",
      "Epoch 7/100 | Batch 39/88 | train_accuracy: 8.494e-01, train_loss: 3.533e-01\n",
      "Epoch 7/100 | Batch 40/88 | train_accuracy: 8.484e-01, train_loss: 3.574e-01\n",
      "Epoch 7/100 | Batch 41/88 | train_accuracy: 8.460e-01, train_loss: 3.600e-01\n",
      "Epoch 7/100 | Batch 42/88 | train_accuracy: 8.467e-01, train_loss: 3.581e-01\n",
      "Epoch 7/100 | Batch 43/88 | train_accuracy: 8.488e-01, train_loss: 3.556e-01\n",
      "Epoch 7/100 | Batch 44/88 | train_accuracy: 8.494e-01, train_loss: 3.615e-01\n",
      "Epoch 7/100 | Batch 45/88 | train_accuracy: 8.514e-01, train_loss: 3.566e-01\n",
      "Epoch 7/100 | Batch 46/88 | train_accuracy: 8.519e-01, train_loss: 3.546e-01\n",
      "Epoch 7/100 | Batch 47/88 | train_accuracy: 8.537e-01, train_loss: 3.538e-01\n",
      "Epoch 7/100 | Batch 48/88 | train_accuracy: 8.516e-01, train_loss: 3.543e-01\n",
      "Epoch 7/100 | Batch 49/88 | train_accuracy: 8.508e-01, train_loss: 3.545e-01\n",
      "Epoch 7/100 | Batch 50/88 | train_accuracy: 8.512e-01, train_loss: 3.530e-01\n",
      "Epoch 7/100 | Batch 51/88 | train_accuracy: 8.493e-01, train_loss: 3.551e-01\n",
      "Epoch 7/100 | Batch 52/88 | train_accuracy: 8.474e-01, train_loss: 3.558e-01\n",
      "Epoch 7/100 | Batch 53/88 | train_accuracy: 8.467e-01, train_loss: 3.541e-01\n",
      "Epoch 7/100 | Batch 54/88 | train_accuracy: 8.484e-01, train_loss: 3.511e-01\n",
      "Epoch 7/100 | Batch 55/88 | train_accuracy: 8.500e-01, train_loss: 3.498e-01\n",
      "Epoch 7/100 | Batch 56/88 | train_accuracy: 8.504e-01, train_loss: 3.499e-01\n",
      "Epoch 7/100 | Batch 57/88 | train_accuracy: 8.520e-01, train_loss: 3.487e-01\n",
      "Epoch 7/100 | Batch 58/88 | train_accuracy: 8.524e-01, train_loss: 3.467e-01\n",
      "Epoch 7/100 | Batch 59/88 | train_accuracy: 8.538e-01, train_loss: 3.443e-01\n",
      "Epoch 7/100 | Batch 60/88 | train_accuracy: 8.552e-01, train_loss: 3.421e-01\n",
      "Epoch 7/100 | Batch 61/88 | train_accuracy: 8.576e-01, train_loss: 3.386e-01\n",
      "Epoch 7/100 | Batch 62/88 | train_accuracy: 8.589e-01, train_loss: 3.372e-01\n",
      "Epoch 7/100 | Batch 63/88 | train_accuracy: 8.581e-01, train_loss: 3.388e-01\n",
      "Epoch 7/100 | Batch 64/88 | train_accuracy: 8.594e-01, train_loss: 3.384e-01\n",
      "Epoch 7/100 | Batch 65/88 | train_accuracy: 8.606e-01, train_loss: 3.364e-01\n",
      "Epoch 7/100 | Batch 66/88 | train_accuracy: 8.608e-01, train_loss: 3.342e-01\n",
      "Epoch 7/100 | Batch 67/88 | train_accuracy: 8.601e-01, train_loss: 3.354e-01\n",
      "Epoch 7/100 | Batch 68/88 | train_accuracy: 8.603e-01, train_loss: 3.375e-01\n",
      "Epoch 7/100 | Batch 69/88 | train_accuracy: 8.596e-01, train_loss: 3.369e-01\n",
      "Epoch 7/100 | Batch 70/88 | train_accuracy: 8.589e-01, train_loss: 3.425e-01\n",
      "Epoch 7/100 | Batch 71/88 | train_accuracy: 8.583e-01, train_loss: 3.427e-01\n",
      "Epoch 7/100 | Batch 72/88 | train_accuracy: 8.594e-01, train_loss: 3.413e-01\n",
      "Epoch 7/100 | Batch 73/88 | train_accuracy: 8.587e-01, train_loss: 3.422e-01\n",
      "Epoch 7/100 | Batch 74/88 | train_accuracy: 8.590e-01, train_loss: 3.433e-01\n",
      "Epoch 7/100 | Batch 75/88 | train_accuracy: 8.575e-01, train_loss: 3.445e-01\n",
      "Epoch 7/100 | Batch 76/88 | train_accuracy: 8.577e-01, train_loss: 3.449e-01\n",
      "Epoch 7/100 | Batch 77/88 | train_accuracy: 8.596e-01, train_loss: 3.426e-01\n",
      "Epoch 7/100 | Batch 78/88 | train_accuracy: 8.582e-01, train_loss: 3.428e-01\n",
      "Epoch 7/100 | Batch 79/88 | train_accuracy: 8.592e-01, train_loss: 3.416e-01\n",
      "Epoch 7/100 | Batch 80/88 | train_accuracy: 8.570e-01, train_loss: 3.446e-01\n",
      "Epoch 7/100 | Batch 81/88 | train_accuracy: 8.565e-01, train_loss: 3.460e-01\n",
      "Epoch 7/100 | Batch 82/88 | train_accuracy: 8.559e-01, train_loss: 3.477e-01\n",
      "Epoch 7/100 | Batch 83/88 | train_accuracy: 8.547e-01, train_loss: 3.480e-01\n",
      "Epoch 7/100 | Batch 84/88 | train_accuracy: 8.542e-01, train_loss: 3.512e-01\n",
      "Epoch 7/100 | Batch 85/88 | train_accuracy: 8.529e-01, train_loss: 3.535e-01\n",
      "Epoch 7/100 | Batch 86/88 | train_accuracy: 8.525e-01, train_loss: 3.542e-01\n",
      "Epoch 7/100 | Batch 87/88 | train_accuracy: 8.506e-01, train_loss: 3.560e-01\n",
      "Epoch 7/100 | Batch 88/88 | train_accuracy: 8.507e-01, train_loss: 3.569e-01\n",
      "Epoch 7/100 | val_accuracy: 6.700e-01, val_loss: 7.532e-01\n",
      "====================\n",
      "Epoch 8/100 | Batch 1/88 | train_accuracy: 9.375e-01, train_loss: 2.599e-01\n",
      "Epoch 8/100 | Batch 2/88 | train_accuracy: 9.062e-01, train_loss: 2.841e-01\n",
      "Epoch 8/100 | Batch 3/88 | train_accuracy: 8.958e-01, train_loss: 3.021e-01\n",
      "Epoch 8/100 | Batch 4/88 | train_accuracy: 9.062e-01, train_loss: 2.701e-01\n",
      "Epoch 8/100 | Batch 5/88 | train_accuracy: 8.750e-01, train_loss: 2.775e-01\n",
      "Epoch 8/100 | Batch 6/88 | train_accuracy: 8.854e-01, train_loss: 2.655e-01\n",
      "Epoch 8/100 | Batch 7/88 | train_accuracy: 8.750e-01, train_loss: 2.727e-01\n",
      "Epoch 8/100 | Batch 8/88 | train_accuracy: 8.750e-01, train_loss: 2.846e-01\n",
      "Epoch 8/100 | Batch 9/88 | train_accuracy: 8.889e-01, train_loss: 2.705e-01\n",
      "Epoch 8/100 | Batch 10/88 | train_accuracy: 8.938e-01, train_loss: 2.592e-01\n",
      "Epoch 8/100 | Batch 11/88 | train_accuracy: 9.034e-01, train_loss: 2.482e-01\n",
      "Epoch 8/100 | Batch 12/88 | train_accuracy: 8.958e-01, train_loss: 2.747e-01\n",
      "Epoch 8/100 | Batch 13/88 | train_accuracy: 9.038e-01, train_loss: 2.598e-01\n",
      "Epoch 8/100 | Batch 14/88 | train_accuracy: 9.062e-01, train_loss: 2.509e-01\n",
      "Epoch 8/100 | Batch 15/88 | train_accuracy: 9.083e-01, train_loss: 2.431e-01\n",
      "Epoch 8/100 | Batch 16/88 | train_accuracy: 9.102e-01, train_loss: 2.397e-01\n",
      "Epoch 8/100 | Batch 17/88 | train_accuracy: 9.081e-01, train_loss: 2.366e-01\n",
      "Epoch 8/100 | Batch 18/88 | train_accuracy: 9.097e-01, train_loss: 2.411e-01\n",
      "Epoch 8/100 | Batch 19/88 | train_accuracy: 9.112e-01, train_loss: 2.348e-01\n",
      "Epoch 8/100 | Batch 20/88 | train_accuracy: 9.031e-01, train_loss: 2.401e-01\n",
      "Epoch 8/100 | Batch 21/88 | train_accuracy: 9.077e-01, train_loss: 2.340e-01\n",
      "Epoch 8/100 | Batch 22/88 | train_accuracy: 9.091e-01, train_loss: 2.305e-01\n",
      "Epoch 8/100 | Batch 23/88 | train_accuracy: 9.076e-01, train_loss: 2.290e-01\n",
      "Epoch 8/100 | Batch 24/88 | train_accuracy: 9.089e-01, train_loss: 2.311e-01\n",
      "Epoch 8/100 | Batch 25/88 | train_accuracy: 9.100e-01, train_loss: 2.338e-01\n",
      "Epoch 8/100 | Batch 26/88 | train_accuracy: 9.135e-01, train_loss: 2.315e-01\n",
      "Epoch 8/100 | Batch 27/88 | train_accuracy: 9.144e-01, train_loss: 2.269e-01\n",
      "Epoch 8/100 | Batch 28/88 | train_accuracy: 9.129e-01, train_loss: 2.330e-01\n",
      "Epoch 8/100 | Batch 29/88 | train_accuracy: 9.138e-01, train_loss: 2.319e-01\n",
      "Epoch 8/100 | Batch 30/88 | train_accuracy: 9.146e-01, train_loss: 2.314e-01\n",
      "Epoch 8/100 | Batch 31/88 | train_accuracy: 9.133e-01, train_loss: 2.360e-01\n",
      "Epoch 8/100 | Batch 32/88 | train_accuracy: 9.121e-01, train_loss: 2.369e-01\n",
      "Epoch 8/100 | Batch 33/88 | train_accuracy: 9.129e-01, train_loss: 2.346e-01\n",
      "Epoch 8/100 | Batch 34/88 | train_accuracy: 9.136e-01, train_loss: 2.351e-01\n",
      "Epoch 8/100 | Batch 35/88 | train_accuracy: 9.143e-01, train_loss: 2.371e-01\n",
      "Epoch 8/100 | Batch 36/88 | train_accuracy: 9.115e-01, train_loss: 2.407e-01\n",
      "Epoch 8/100 | Batch 37/88 | train_accuracy: 9.105e-01, train_loss: 2.432e-01\n",
      "Epoch 8/100 | Batch 38/88 | train_accuracy: 9.128e-01, train_loss: 2.406e-01\n",
      "Epoch 8/100 | Batch 39/88 | train_accuracy: 9.151e-01, train_loss: 2.394e-01\n",
      "Epoch 8/100 | Batch 40/88 | train_accuracy: 9.141e-01, train_loss: 2.399e-01\n",
      "Epoch 8/100 | Batch 41/88 | train_accuracy: 9.131e-01, train_loss: 2.407e-01\n",
      "Epoch 8/100 | Batch 42/88 | train_accuracy: 9.137e-01, train_loss: 2.414e-01\n",
      "Epoch 8/100 | Batch 43/88 | train_accuracy: 9.099e-01, train_loss: 2.459e-01\n",
      "Epoch 8/100 | Batch 44/88 | train_accuracy: 9.105e-01, train_loss: 2.431e-01\n",
      "Epoch 8/100 | Batch 45/88 | train_accuracy: 9.069e-01, train_loss: 2.482e-01\n",
      "Epoch 8/100 | Batch 46/88 | train_accuracy: 9.062e-01, train_loss: 2.503e-01\n",
      "Epoch 8/100 | Batch 47/88 | train_accuracy: 9.056e-01, train_loss: 2.499e-01\n",
      "Epoch 8/100 | Batch 48/88 | train_accuracy: 9.062e-01, train_loss: 2.492e-01\n",
      "Epoch 8/100 | Batch 49/88 | train_accuracy: 9.069e-01, train_loss: 2.488e-01\n",
      "Epoch 8/100 | Batch 50/88 | train_accuracy: 9.062e-01, train_loss: 2.508e-01\n",
      "Epoch 8/100 | Batch 51/88 | train_accuracy: 9.069e-01, train_loss: 2.505e-01\n",
      "Epoch 8/100 | Batch 52/88 | train_accuracy: 9.050e-01, train_loss: 2.519e-01\n",
      "Epoch 8/100 | Batch 53/88 | train_accuracy: 9.057e-01, train_loss: 2.504e-01\n",
      "Epoch 8/100 | Batch 54/88 | train_accuracy: 9.062e-01, train_loss: 2.524e-01\n",
      "Epoch 8/100 | Batch 55/88 | train_accuracy: 9.068e-01, train_loss: 2.525e-01\n",
      "Epoch 8/100 | Batch 56/88 | train_accuracy: 9.085e-01, train_loss: 2.503e-01\n",
      "Epoch 8/100 | Batch 57/88 | train_accuracy: 9.068e-01, train_loss: 2.516e-01\n",
      "Epoch 8/100 | Batch 58/88 | train_accuracy: 9.041e-01, train_loss: 2.549e-01\n",
      "Epoch 8/100 | Batch 59/88 | train_accuracy: 9.057e-01, train_loss: 2.528e-01\n",
      "Epoch 8/100 | Batch 60/88 | train_accuracy: 9.073e-01, train_loss: 2.504e-01\n",
      "Epoch 8/100 | Batch 61/88 | train_accuracy: 9.057e-01, train_loss: 2.536e-01\n",
      "Epoch 8/100 | Batch 62/88 | train_accuracy: 9.042e-01, train_loss: 2.557e-01\n",
      "Epoch 8/100 | Batch 63/88 | train_accuracy: 9.008e-01, train_loss: 2.590e-01\n",
      "Epoch 8/100 | Batch 64/88 | train_accuracy: 8.994e-01, train_loss: 2.628e-01\n",
      "Epoch 8/100 | Batch 65/88 | train_accuracy: 9.000e-01, train_loss: 2.614e-01\n",
      "Epoch 8/100 | Batch 66/88 | train_accuracy: 8.977e-01, train_loss: 2.646e-01\n",
      "Epoch 8/100 | Batch 67/88 | train_accuracy: 8.955e-01, train_loss: 2.684e-01\n",
      "Epoch 8/100 | Batch 68/88 | train_accuracy: 8.952e-01, train_loss: 2.713e-01\n",
      "Epoch 8/100 | Batch 69/88 | train_accuracy: 8.922e-01, train_loss: 2.762e-01\n",
      "Epoch 8/100 | Batch 70/88 | train_accuracy: 8.902e-01, train_loss: 2.788e-01\n",
      "Epoch 8/100 | Batch 71/88 | train_accuracy: 8.882e-01, train_loss: 2.815e-01\n",
      "Epoch 8/100 | Batch 72/88 | train_accuracy: 8.880e-01, train_loss: 2.816e-01\n",
      "Epoch 8/100 | Batch 73/88 | train_accuracy: 8.870e-01, train_loss: 2.830e-01\n",
      "Epoch 8/100 | Batch 74/88 | train_accuracy: 8.885e-01, train_loss: 2.821e-01\n",
      "Epoch 8/100 | Batch 75/88 | train_accuracy: 8.883e-01, train_loss: 2.823e-01\n",
      "Epoch 8/100 | Batch 76/88 | train_accuracy: 8.882e-01, train_loss: 2.845e-01\n",
      "Epoch 8/100 | Batch 77/88 | train_accuracy: 8.880e-01, train_loss: 2.848e-01\n",
      "Epoch 8/100 | Batch 78/88 | train_accuracy: 8.862e-01, train_loss: 2.866e-01\n",
      "Epoch 8/100 | Batch 79/88 | train_accuracy: 8.837e-01, train_loss: 2.895e-01\n",
      "Epoch 8/100 | Batch 80/88 | train_accuracy: 8.836e-01, train_loss: 2.904e-01\n",
      "Epoch 8/100 | Batch 81/88 | train_accuracy: 8.835e-01, train_loss: 2.921e-01\n",
      "Epoch 8/100 | Batch 82/88 | train_accuracy: 8.841e-01, train_loss: 2.919e-01\n",
      "Epoch 8/100 | Batch 83/88 | train_accuracy: 8.840e-01, train_loss: 2.921e-01\n",
      "Epoch 8/100 | Batch 84/88 | train_accuracy: 8.847e-01, train_loss: 2.914e-01\n",
      "Epoch 8/100 | Batch 85/88 | train_accuracy: 8.860e-01, train_loss: 2.898e-01\n",
      "Epoch 8/100 | Batch 86/88 | train_accuracy: 8.844e-01, train_loss: 2.930e-01\n",
      "Epoch 8/100 | Batch 87/88 | train_accuracy: 8.851e-01, train_loss: 2.917e-01\n",
      "Epoch 8/100 | Batch 88/88 | train_accuracy: 8.857e-01, train_loss: 2.894e-01\n",
      "Epoch 8/100 | val_accuracy: 6.900e-01, val_loss: 6.940e-01\n",
      "====================\n",
      "Epoch 9/100 | Batch 1/88 | train_accuracy: 1.000e+00, train_loss: 1.139e-01\n",
      "Epoch 9/100 | Batch 2/88 | train_accuracy: 9.375e-01, train_loss: 1.919e-01\n",
      "Epoch 9/100 | Batch 3/88 | train_accuracy: 9.375e-01, train_loss: 2.271e-01\n",
      "Epoch 9/100 | Batch 4/88 | train_accuracy: 9.375e-01, train_loss: 2.149e-01\n",
      "Epoch 9/100 | Batch 5/88 | train_accuracy: 9.375e-01, train_loss: 2.027e-01\n",
      "Epoch 9/100 | Batch 6/88 | train_accuracy: 9.375e-01, train_loss: 1.956e-01\n",
      "Epoch 9/100 | Batch 7/88 | train_accuracy: 9.375e-01, train_loss: 2.029e-01\n",
      "Epoch 9/100 | Batch 8/88 | train_accuracy: 9.219e-01, train_loss: 2.159e-01\n",
      "Epoch 9/100 | Batch 9/88 | train_accuracy: 9.236e-01, train_loss: 2.103e-01\n",
      "Epoch 9/100 | Batch 10/88 | train_accuracy: 9.313e-01, train_loss: 2.014e-01\n",
      "Epoch 9/100 | Batch 11/88 | train_accuracy: 9.318e-01, train_loss: 1.967e-01\n",
      "Epoch 9/100 | Batch 12/88 | train_accuracy: 9.219e-01, train_loss: 2.089e-01\n",
      "Epoch 9/100 | Batch 13/88 | train_accuracy: 9.279e-01, train_loss: 1.981e-01\n",
      "Epoch 9/100 | Batch 14/88 | train_accuracy: 9.241e-01, train_loss: 1.963e-01\n",
      "Epoch 9/100 | Batch 15/88 | train_accuracy: 9.250e-01, train_loss: 1.902e-01\n",
      "Epoch 9/100 | Batch 16/88 | train_accuracy: 9.219e-01, train_loss: 1.885e-01\n",
      "Epoch 9/100 | Batch 17/88 | train_accuracy: 9.265e-01, train_loss: 1.797e-01\n",
      "Epoch 9/100 | Batch 18/88 | train_accuracy: 9.236e-01, train_loss: 1.784e-01\n",
      "Epoch 9/100 | Batch 19/88 | train_accuracy: 9.211e-01, train_loss: 1.814e-01\n",
      "Epoch 9/100 | Batch 20/88 | train_accuracy: 9.219e-01, train_loss: 1.765e-01\n",
      "Epoch 9/100 | Batch 21/88 | train_accuracy: 9.196e-01, train_loss: 1.760e-01\n",
      "Epoch 9/100 | Batch 22/88 | train_accuracy: 9.176e-01, train_loss: 1.794e-01\n",
      "Epoch 9/100 | Batch 23/88 | train_accuracy: 9.185e-01, train_loss: 1.767e-01\n",
      "Epoch 9/100 | Batch 24/88 | train_accuracy: 9.167e-01, train_loss: 1.806e-01\n",
      "Epoch 9/100 | Batch 25/88 | train_accuracy: 9.175e-01, train_loss: 1.875e-01\n",
      "Epoch 9/100 | Batch 26/88 | train_accuracy: 9.183e-01, train_loss: 1.847e-01\n",
      "Epoch 9/100 | Batch 27/88 | train_accuracy: 9.213e-01, train_loss: 1.805e-01\n",
      "Epoch 9/100 | Batch 28/88 | train_accuracy: 9.219e-01, train_loss: 1.804e-01\n",
      "Epoch 9/100 | Batch 29/88 | train_accuracy: 9.246e-01, train_loss: 1.774e-01\n",
      "Epoch 9/100 | Batch 30/88 | train_accuracy: 9.187e-01, train_loss: 1.888e-01\n",
      "Epoch 9/100 | Batch 31/88 | train_accuracy: 9.173e-01, train_loss: 1.917e-01\n",
      "Epoch 9/100 | Batch 32/88 | train_accuracy: 9.160e-01, train_loss: 1.936e-01\n",
      "Epoch 9/100 | Batch 33/88 | train_accuracy: 9.186e-01, train_loss: 1.908e-01\n",
      "Epoch 9/100 | Batch 34/88 | train_accuracy: 9.191e-01, train_loss: 1.907e-01\n",
      "Epoch 9/100 | Batch 35/88 | train_accuracy: 9.179e-01, train_loss: 1.918e-01\n",
      "Epoch 9/100 | Batch 36/88 | train_accuracy: 9.167e-01, train_loss: 1.927e-01\n",
      "Epoch 9/100 | Batch 37/88 | train_accuracy: 9.189e-01, train_loss: 1.918e-01\n",
      "Epoch 9/100 | Batch 38/88 | train_accuracy: 9.178e-01, train_loss: 1.943e-01\n",
      "Epoch 9/100 | Batch 39/88 | train_accuracy: 9.183e-01, train_loss: 1.953e-01\n",
      "Epoch 9/100 | Batch 40/88 | train_accuracy: 9.172e-01, train_loss: 1.954e-01\n",
      "Epoch 9/100 | Batch 41/88 | train_accuracy: 9.177e-01, train_loss: 1.950e-01\n",
      "Epoch 9/100 | Batch 42/88 | train_accuracy: 9.182e-01, train_loss: 1.967e-01\n",
      "Epoch 9/100 | Batch 43/88 | train_accuracy: 9.201e-01, train_loss: 1.937e-01\n",
      "Epoch 9/100 | Batch 44/88 | train_accuracy: 9.219e-01, train_loss: 1.918e-01\n",
      "Epoch 9/100 | Batch 45/88 | train_accuracy: 9.222e-01, train_loss: 1.931e-01\n",
      "Epoch 9/100 | Batch 46/88 | train_accuracy: 9.226e-01, train_loss: 1.912e-01\n",
      "Epoch 9/100 | Batch 47/88 | train_accuracy: 9.242e-01, train_loss: 1.883e-01\n",
      "Epoch 9/100 | Batch 48/88 | train_accuracy: 9.245e-01, train_loss: 1.878e-01\n",
      "Epoch 9/100 | Batch 49/88 | train_accuracy: 9.235e-01, train_loss: 1.908e-01\n",
      "Epoch 9/100 | Batch 50/88 | train_accuracy: 9.250e-01, train_loss: 1.887e-01\n",
      "Epoch 9/100 | Batch 51/88 | train_accuracy: 9.252e-01, train_loss: 1.888e-01\n",
      "Epoch 9/100 | Batch 52/88 | train_accuracy: 9.255e-01, train_loss: 1.891e-01\n",
      "Epoch 9/100 | Batch 53/88 | train_accuracy: 9.222e-01, train_loss: 1.943e-01\n",
      "Epoch 9/100 | Batch 54/88 | train_accuracy: 9.225e-01, train_loss: 1.936e-01\n",
      "Epoch 9/100 | Batch 55/88 | train_accuracy: 9.239e-01, train_loss: 1.918e-01\n",
      "Epoch 9/100 | Batch 56/88 | train_accuracy: 9.219e-01, train_loss: 1.933e-01\n",
      "Epoch 9/100 | Batch 57/88 | train_accuracy: 9.211e-01, train_loss: 1.971e-01\n",
      "Epoch 9/100 | Batch 58/88 | train_accuracy: 9.192e-01, train_loss: 1.991e-01\n",
      "Epoch 9/100 | Batch 59/88 | train_accuracy: 9.206e-01, train_loss: 1.982e-01\n",
      "Epoch 9/100 | Batch 60/88 | train_accuracy: 9.198e-01, train_loss: 1.981e-01\n",
      "Epoch 9/100 | Batch 61/88 | train_accuracy: 9.170e-01, train_loss: 2.037e-01\n",
      "Epoch 9/100 | Batch 62/88 | train_accuracy: 9.153e-01, train_loss: 2.074e-01\n",
      "Epoch 9/100 | Batch 63/88 | train_accuracy: 9.137e-01, train_loss: 2.094e-01\n",
      "Epoch 9/100 | Batch 64/88 | train_accuracy: 9.121e-01, train_loss: 2.145e-01\n",
      "Epoch 9/100 | Batch 65/88 | train_accuracy: 9.125e-01, train_loss: 2.144e-01\n",
      "Epoch 9/100 | Batch 66/88 | train_accuracy: 9.119e-01, train_loss: 2.147e-01\n",
      "Epoch 9/100 | Batch 67/88 | train_accuracy: 9.123e-01, train_loss: 2.144e-01\n",
      "Epoch 9/100 | Batch 68/88 | train_accuracy: 9.108e-01, train_loss: 2.190e-01\n",
      "Epoch 9/100 | Batch 69/88 | train_accuracy: 9.121e-01, train_loss: 2.172e-01\n",
      "Epoch 9/100 | Batch 70/88 | train_accuracy: 9.134e-01, train_loss: 2.150e-01\n",
      "Epoch 9/100 | Batch 71/88 | train_accuracy: 9.137e-01, train_loss: 2.146e-01\n",
      "Epoch 9/100 | Batch 72/88 | train_accuracy: 9.106e-01, train_loss: 2.185e-01\n",
      "Epoch 9/100 | Batch 73/88 | train_accuracy: 9.092e-01, train_loss: 2.207e-01\n",
      "Epoch 9/100 | Batch 74/88 | train_accuracy: 9.096e-01, train_loss: 2.195e-01\n",
      "Epoch 9/100 | Batch 75/88 | train_accuracy: 9.092e-01, train_loss: 2.207e-01\n",
      "Epoch 9/100 | Batch 76/88 | train_accuracy: 9.095e-01, train_loss: 2.198e-01\n",
      "Epoch 9/100 | Batch 77/88 | train_accuracy: 9.091e-01, train_loss: 2.205e-01\n",
      "Epoch 9/100 | Batch 78/88 | train_accuracy: 9.095e-01, train_loss: 2.205e-01\n",
      "Epoch 9/100 | Batch 79/88 | train_accuracy: 9.066e-01, train_loss: 2.244e-01\n",
      "Epoch 9/100 | Batch 80/88 | train_accuracy: 9.070e-01, train_loss: 2.250e-01\n",
      "Epoch 9/100 | Batch 81/88 | train_accuracy: 9.082e-01, train_loss: 2.246e-01\n",
      "Epoch 9/100 | Batch 82/88 | train_accuracy: 9.078e-01, train_loss: 2.248e-01\n",
      "Epoch 9/100 | Batch 83/88 | train_accuracy: 9.089e-01, train_loss: 2.236e-01\n",
      "Epoch 9/100 | Batch 84/88 | train_accuracy: 9.085e-01, train_loss: 2.238e-01\n",
      "Epoch 9/100 | Batch 85/88 | train_accuracy: 9.088e-01, train_loss: 2.232e-01\n",
      "Epoch 9/100 | Batch 86/88 | train_accuracy: 9.084e-01, train_loss: 2.232e-01\n",
      "Epoch 9/100 | Batch 87/88 | train_accuracy: 9.080e-01, train_loss: 2.234e-01\n",
      "Epoch 9/100 | Batch 88/88 | train_accuracy: 9.079e-01, train_loss: 2.280e-01\n",
      "Epoch 9/100 | val_accuracy: 6.700e-01, val_loss: 8.607e-01\n",
      "====================\n",
      "Epoch 10/100 | Batch 1/88 | train_accuracy: 8.750e-01, train_loss: 2.343e-01\n",
      "Epoch 10/100 | Batch 2/88 | train_accuracy: 9.375e-01, train_loss: 1.907e-01\n",
      "Epoch 10/100 | Batch 3/88 | train_accuracy: 9.583e-01, train_loss: 1.777e-01\n",
      "Epoch 10/100 | Batch 4/88 | train_accuracy: 9.531e-01, train_loss: 1.664e-01\n",
      "Epoch 10/100 | Batch 5/88 | train_accuracy: 9.625e-01, train_loss: 1.531e-01\n",
      "Epoch 10/100 | Batch 6/88 | train_accuracy: 9.688e-01, train_loss: 1.494e-01\n",
      "Epoch 10/100 | Batch 7/88 | train_accuracy: 9.554e-01, train_loss: 1.640e-01\n",
      "Epoch 10/100 | Batch 8/88 | train_accuracy: 9.609e-01, train_loss: 1.632e-01\n",
      "Epoch 10/100 | Batch 9/88 | train_accuracy: 9.514e-01, train_loss: 1.710e-01\n",
      "Epoch 10/100 | Batch 10/88 | train_accuracy: 9.500e-01, train_loss: 1.690e-01\n",
      "Epoch 10/100 | Batch 11/88 | train_accuracy: 9.432e-01, train_loss: 1.709e-01\n",
      "Epoch 10/100 | Batch 12/88 | train_accuracy: 9.375e-01, train_loss: 1.690e-01\n",
      "Epoch 10/100 | Batch 13/88 | train_accuracy: 9.327e-01, train_loss: 1.695e-01\n",
      "Epoch 10/100 | Batch 14/88 | train_accuracy: 9.330e-01, train_loss: 1.654e-01\n",
      "Epoch 10/100 | Batch 15/88 | train_accuracy: 9.375e-01, train_loss: 1.587e-01\n",
      "Epoch 10/100 | Batch 16/88 | train_accuracy: 9.414e-01, train_loss: 1.538e-01\n",
      "Epoch 10/100 | Batch 17/88 | train_accuracy: 9.449e-01, train_loss: 1.504e-01\n",
      "Epoch 10/100 | Batch 18/88 | train_accuracy: 9.444e-01, train_loss: 1.467e-01\n",
      "Epoch 10/100 | Batch 19/88 | train_accuracy: 9.474e-01, train_loss: 1.417e-01\n",
      "Epoch 10/100 | Batch 20/88 | train_accuracy: 9.500e-01, train_loss: 1.377e-01\n",
      "Epoch 10/100 | Batch 21/88 | train_accuracy: 9.494e-01, train_loss: 1.366e-01\n",
      "Epoch 10/100 | Batch 22/88 | train_accuracy: 9.489e-01, train_loss: 1.336e-01\n",
      "Epoch 10/100 | Batch 23/88 | train_accuracy: 9.484e-01, train_loss: 1.356e-01\n",
      "Epoch 10/100 | Batch 24/88 | train_accuracy: 9.505e-01, train_loss: 1.311e-01\n",
      "Epoch 10/100 | Batch 25/88 | train_accuracy: 9.500e-01, train_loss: 1.320e-01\n",
      "Epoch 10/100 | Batch 26/88 | train_accuracy: 9.495e-01, train_loss: 1.306e-01\n",
      "Epoch 10/100 | Batch 27/88 | train_accuracy: 9.468e-01, train_loss: 1.303e-01\n",
      "Epoch 10/100 | Batch 28/88 | train_accuracy: 9.442e-01, train_loss: 1.311e-01\n",
      "Epoch 10/100 | Batch 29/88 | train_accuracy: 9.418e-01, train_loss: 1.304e-01\n",
      "Epoch 10/100 | Batch 30/88 | train_accuracy: 9.437e-01, train_loss: 1.279e-01\n",
      "Epoch 10/100 | Batch 31/88 | train_accuracy: 9.415e-01, train_loss: 1.291e-01\n",
      "Epoch 10/100 | Batch 32/88 | train_accuracy: 9.434e-01, train_loss: 1.274e-01\n",
      "Epoch 10/100 | Batch 33/88 | train_accuracy: 9.451e-01, train_loss: 1.239e-01\n",
      "Epoch 10/100 | Batch 34/88 | train_accuracy: 9.449e-01, train_loss: 1.250e-01\n",
      "Epoch 10/100 | Batch 35/88 | train_accuracy: 9.393e-01, train_loss: 1.343e-01\n",
      "Epoch 10/100 | Batch 36/88 | train_accuracy: 9.410e-01, train_loss: 1.335e-01\n",
      "Epoch 10/100 | Batch 37/88 | train_accuracy: 9.392e-01, train_loss: 1.378e-01\n",
      "Epoch 10/100 | Batch 38/88 | train_accuracy: 9.375e-01, train_loss: 1.390e-01\n",
      "Epoch 10/100 | Batch 39/88 | train_accuracy: 9.391e-01, train_loss: 1.376e-01\n",
      "Epoch 10/100 | Batch 40/88 | train_accuracy: 9.391e-01, train_loss: 1.389e-01\n",
      "Epoch 10/100 | Batch 41/88 | train_accuracy: 9.390e-01, train_loss: 1.468e-01\n",
      "Epoch 10/100 | Batch 42/88 | train_accuracy: 9.405e-01, train_loss: 1.457e-01\n",
      "Epoch 10/100 | Batch 43/88 | train_accuracy: 9.419e-01, train_loss: 1.430e-01\n",
      "Epoch 10/100 | Batch 44/88 | train_accuracy: 9.403e-01, train_loss: 1.466e-01\n",
      "Epoch 10/100 | Batch 45/88 | train_accuracy: 9.403e-01, train_loss: 1.458e-01\n",
      "Epoch 10/100 | Batch 46/88 | train_accuracy: 9.361e-01, train_loss: 1.553e-01\n",
      "Epoch 10/100 | Batch 47/88 | train_accuracy: 9.362e-01, train_loss: 1.547e-01\n",
      "Epoch 10/100 | Batch 48/88 | train_accuracy: 9.336e-01, train_loss: 1.592e-01\n",
      "Epoch 10/100 | Batch 49/88 | train_accuracy: 9.324e-01, train_loss: 1.594e-01\n",
      "Epoch 10/100 | Batch 50/88 | train_accuracy: 9.313e-01, train_loss: 1.606e-01\n",
      "Epoch 10/100 | Batch 51/88 | train_accuracy: 9.314e-01, train_loss: 1.606e-01\n",
      "Epoch 10/100 | Batch 52/88 | train_accuracy: 9.315e-01, train_loss: 1.588e-01\n",
      "Epoch 10/100 | Batch 53/88 | train_accuracy: 9.328e-01, train_loss: 1.575e-01\n",
      "Epoch 10/100 | Batch 54/88 | train_accuracy: 9.340e-01, train_loss: 1.553e-01\n",
      "Epoch 10/100 | Batch 55/88 | train_accuracy: 9.341e-01, train_loss: 1.563e-01\n",
      "Epoch 10/100 | Batch 56/88 | train_accuracy: 9.342e-01, train_loss: 1.571e-01\n",
      "Epoch 10/100 | Batch 57/88 | train_accuracy: 9.342e-01, train_loss: 1.558e-01\n",
      "Epoch 10/100 | Batch 58/88 | train_accuracy: 9.353e-01, train_loss: 1.560e-01\n",
      "Epoch 10/100 | Batch 59/88 | train_accuracy: 9.343e-01, train_loss: 1.574e-01\n",
      "Epoch 10/100 | Batch 60/88 | train_accuracy: 9.354e-01, train_loss: 1.555e-01\n",
      "Epoch 10/100 | Batch 61/88 | train_accuracy: 9.355e-01, train_loss: 1.580e-01\n",
      "Epoch 10/100 | Batch 62/88 | train_accuracy: 9.355e-01, train_loss: 1.569e-01\n",
      "Epoch 10/100 | Batch 63/88 | train_accuracy: 9.345e-01, train_loss: 1.578e-01\n",
      "Epoch 10/100 | Batch 64/88 | train_accuracy: 9.336e-01, train_loss: 1.605e-01\n",
      "Epoch 10/100 | Batch 65/88 | train_accuracy: 9.346e-01, train_loss: 1.594e-01\n",
      "Epoch 10/100 | Batch 66/88 | train_accuracy: 9.328e-01, train_loss: 1.633e-01\n",
      "Epoch 10/100 | Batch 67/88 | train_accuracy: 9.328e-01, train_loss: 1.639e-01\n",
      "Epoch 10/100 | Batch 68/88 | train_accuracy: 9.338e-01, train_loss: 1.625e-01\n",
      "Epoch 10/100 | Batch 69/88 | train_accuracy: 9.339e-01, train_loss: 1.632e-01\n",
      "Epoch 10/100 | Batch 70/88 | train_accuracy: 9.304e-01, train_loss: 1.680e-01\n",
      "Epoch 10/100 | Batch 71/88 | train_accuracy: 9.313e-01, train_loss: 1.675e-01\n",
      "Epoch 10/100 | Batch 72/88 | train_accuracy: 9.314e-01, train_loss: 1.683e-01\n",
      "Epoch 10/100 | Batch 73/88 | train_accuracy: 9.315e-01, train_loss: 1.684e-01\n",
      "Epoch 10/100 | Batch 74/88 | train_accuracy: 9.307e-01, train_loss: 1.684e-01\n",
      "Epoch 10/100 | Batch 75/88 | train_accuracy: 9.308e-01, train_loss: 1.713e-01\n",
      "Epoch 10/100 | Batch 76/88 | train_accuracy: 9.309e-01, train_loss: 1.714e-01\n",
      "Epoch 10/100 | Batch 77/88 | train_accuracy: 9.318e-01, train_loss: 1.708e-01\n",
      "Epoch 10/100 | Batch 78/88 | train_accuracy: 9.303e-01, train_loss: 1.770e-01\n",
      "Epoch 10/100 | Batch 79/88 | train_accuracy: 9.296e-01, train_loss: 1.816e-01\n",
      "Epoch 10/100 | Batch 80/88 | train_accuracy: 9.281e-01, train_loss: 1.836e-01\n",
      "Epoch 10/100 | Batch 81/88 | train_accuracy: 9.259e-01, train_loss: 1.885e-01\n",
      "Epoch 10/100 | Batch 82/88 | train_accuracy: 9.253e-01, train_loss: 1.888e-01\n",
      "Epoch 10/100 | Batch 83/88 | train_accuracy: 9.262e-01, train_loss: 1.873e-01\n",
      "Epoch 10/100 | Batch 84/88 | train_accuracy: 9.263e-01, train_loss: 1.878e-01\n",
      "Epoch 10/100 | Batch 85/88 | train_accuracy: 9.265e-01, train_loss: 1.876e-01\n",
      "Epoch 10/100 | Batch 86/88 | train_accuracy: 9.251e-01, train_loss: 1.901e-01\n",
      "Epoch 10/100 | Batch 87/88 | train_accuracy: 9.224e-01, train_loss: 1.924e-01\n",
      "Epoch 10/100 | Batch 88/88 | train_accuracy: 9.221e-01, train_loss: 1.933e-01\n",
      "Epoch 10/100 | val_accuracy: 7.000e-01, val_loss: 7.765e-01\n",
      "====================\n",
      "Epoch 11/100 | Batch 1/88 | train_accuracy: 1.000e+00, train_loss: 1.194e-01\n",
      "Epoch 11/100 | Batch 2/88 | train_accuracy: 1.000e+00, train_loss: 1.068e-01\n",
      "Epoch 11/100 | Batch 3/88 | train_accuracy: 1.000e+00, train_loss: 9.004e-02\n",
      "Epoch 11/100 | Batch 4/88 | train_accuracy: 1.000e+00, train_loss: 1.025e-01\n",
      "Epoch 11/100 | Batch 5/88 | train_accuracy: 1.000e+00, train_loss: 1.105e-01\n",
      "Epoch 11/100 | Batch 6/88 | train_accuracy: 9.896e-01, train_loss: 1.167e-01\n",
      "Epoch 11/100 | Batch 7/88 | train_accuracy: 9.732e-01, train_loss: 1.389e-01\n",
      "Epoch 11/100 | Batch 8/88 | train_accuracy: 9.766e-01, train_loss: 1.306e-01\n",
      "Epoch 11/100 | Batch 9/88 | train_accuracy: 9.722e-01, train_loss: 1.414e-01\n",
      "Epoch 11/100 | Batch 10/88 | train_accuracy: 9.688e-01, train_loss: 1.463e-01\n",
      "Epoch 11/100 | Batch 11/88 | train_accuracy: 9.659e-01, train_loss: 1.496e-01\n",
      "Epoch 11/100 | Batch 12/88 | train_accuracy: 9.635e-01, train_loss: 1.508e-01\n",
      "Epoch 11/100 | Batch 13/88 | train_accuracy: 9.615e-01, train_loss: 1.520e-01\n",
      "Epoch 11/100 | Batch 14/88 | train_accuracy: 9.598e-01, train_loss: 1.506e-01\n",
      "Epoch 11/100 | Batch 15/88 | train_accuracy: 9.583e-01, train_loss: 1.476e-01\n",
      "Epoch 11/100 | Batch 16/88 | train_accuracy: 9.609e-01, train_loss: 1.411e-01\n",
      "Epoch 11/100 | Batch 17/88 | train_accuracy: 9.559e-01, train_loss: 1.602e-01\n",
      "Epoch 11/100 | Batch 18/88 | train_accuracy: 9.479e-01, train_loss: 1.713e-01\n",
      "Epoch 11/100 | Batch 19/88 | train_accuracy: 9.507e-01, train_loss: 1.664e-01\n",
      "Epoch 11/100 | Batch 20/88 | train_accuracy: 9.500e-01, train_loss: 1.669e-01\n",
      "Epoch 11/100 | Batch 21/88 | train_accuracy: 9.524e-01, train_loss: 1.598e-01\n",
      "Epoch 11/100 | Batch 22/88 | train_accuracy: 9.517e-01, train_loss: 1.583e-01\n",
      "Epoch 11/100 | Batch 23/88 | train_accuracy: 9.429e-01, train_loss: 1.698e-01\n",
      "Epoch 11/100 | Batch 24/88 | train_accuracy: 9.453e-01, train_loss: 1.660e-01\n",
      "Epoch 11/100 | Batch 25/88 | train_accuracy: 9.400e-01, train_loss: 1.720e-01\n",
      "Epoch 11/100 | Batch 26/88 | train_accuracy: 9.375e-01, train_loss: 1.734e-01\n",
      "Epoch 11/100 | Batch 27/88 | train_accuracy: 9.398e-01, train_loss: 1.696e-01\n",
      "Epoch 11/100 | Batch 28/88 | train_accuracy: 9.375e-01, train_loss: 1.720e-01\n",
      "Epoch 11/100 | Batch 29/88 | train_accuracy: 9.397e-01, train_loss: 1.675e-01\n",
      "Epoch 11/100 | Batch 30/88 | train_accuracy: 9.396e-01, train_loss: 1.678e-01\n",
      "Epoch 11/100 | Batch 31/88 | train_accuracy: 9.395e-01, train_loss: 1.668e-01\n",
      "Epoch 11/100 | Batch 32/88 | train_accuracy: 9.375e-01, train_loss: 1.671e-01\n",
      "Epoch 11/100 | Batch 33/88 | train_accuracy: 9.394e-01, train_loss: 1.635e-01\n",
      "Epoch 11/100 | Batch 34/88 | train_accuracy: 9.412e-01, train_loss: 1.596e-01\n",
      "Epoch 11/100 | Batch 35/88 | train_accuracy: 9.429e-01, train_loss: 1.604e-01\n",
      "Epoch 11/100 | Batch 36/88 | train_accuracy: 9.427e-01, train_loss: 1.602e-01\n",
      "Epoch 11/100 | Batch 37/88 | train_accuracy: 9.443e-01, train_loss: 1.585e-01\n",
      "Epoch 11/100 | Batch 38/88 | train_accuracy: 9.457e-01, train_loss: 1.573e-01\n",
      "Epoch 11/100 | Batch 39/88 | train_accuracy: 9.471e-01, train_loss: 1.550e-01\n",
      "Epoch 11/100 | Batch 40/88 | train_accuracy: 9.484e-01, train_loss: 1.526e-01\n",
      "Epoch 11/100 | Batch 41/88 | train_accuracy: 9.497e-01, train_loss: 1.507e-01\n",
      "Epoch 11/100 | Batch 42/88 | train_accuracy: 9.509e-01, train_loss: 1.482e-01\n",
      "Epoch 11/100 | Batch 43/88 | train_accuracy: 9.491e-01, train_loss: 1.502e-01\n",
      "Epoch 11/100 | Batch 44/88 | train_accuracy: 9.489e-01, train_loss: 1.493e-01\n",
      "Epoch 11/100 | Batch 45/88 | train_accuracy: 9.500e-01, train_loss: 1.472e-01\n",
      "Epoch 11/100 | Batch 46/88 | train_accuracy: 9.511e-01, train_loss: 1.461e-01\n",
      "Epoch 11/100 | Batch 47/88 | train_accuracy: 9.521e-01, train_loss: 1.443e-01\n",
      "Epoch 11/100 | Batch 48/88 | train_accuracy: 9.518e-01, train_loss: 1.427e-01\n",
      "Epoch 11/100 | Batch 49/88 | train_accuracy: 9.528e-01, train_loss: 1.410e-01\n",
      "Epoch 11/100 | Batch 50/88 | train_accuracy: 9.537e-01, train_loss: 1.395e-01\n",
      "Epoch 11/100 | Batch 51/88 | train_accuracy: 9.547e-01, train_loss: 1.387e-01\n",
      "Epoch 11/100 | Batch 52/88 | train_accuracy: 9.555e-01, train_loss: 1.376e-01\n",
      "Epoch 11/100 | Batch 53/88 | train_accuracy: 9.564e-01, train_loss: 1.360e-01\n",
      "Epoch 11/100 | Batch 54/88 | train_accuracy: 9.549e-01, train_loss: 1.384e-01\n",
      "Epoch 11/100 | Batch 55/88 | train_accuracy: 9.557e-01, train_loss: 1.369e-01\n",
      "Epoch 11/100 | Batch 56/88 | train_accuracy: 9.554e-01, train_loss: 1.361e-01\n",
      "Epoch 11/100 | Batch 57/88 | train_accuracy: 9.550e-01, train_loss: 1.358e-01\n",
      "Epoch 11/100 | Batch 58/88 | train_accuracy: 9.558e-01, train_loss: 1.338e-01\n",
      "Epoch 11/100 | Batch 59/88 | train_accuracy: 9.555e-01, train_loss: 1.328e-01\n",
      "Epoch 11/100 | Batch 60/88 | train_accuracy: 9.521e-01, train_loss: 1.389e-01\n",
      "Epoch 11/100 | Batch 61/88 | train_accuracy: 9.518e-01, train_loss: 1.379e-01\n",
      "Epoch 11/100 | Batch 62/88 | train_accuracy: 9.516e-01, train_loss: 1.387e-01\n",
      "Epoch 11/100 | Batch 63/88 | train_accuracy: 9.494e-01, train_loss: 1.403e-01\n",
      "Epoch 11/100 | Batch 64/88 | train_accuracy: 9.492e-01, train_loss: 1.399e-01\n",
      "Epoch 11/100 | Batch 65/88 | train_accuracy: 9.490e-01, train_loss: 1.391e-01\n",
      "Epoch 11/100 | Batch 66/88 | train_accuracy: 9.489e-01, train_loss: 1.382e-01\n",
      "Epoch 11/100 | Batch 67/88 | train_accuracy: 9.478e-01, train_loss: 1.388e-01\n",
      "Epoch 11/100 | Batch 68/88 | train_accuracy: 9.476e-01, train_loss: 1.394e-01\n",
      "Epoch 11/100 | Batch 69/88 | train_accuracy: 9.475e-01, train_loss: 1.397e-01\n",
      "Epoch 11/100 | Batch 70/88 | train_accuracy: 9.473e-01, train_loss: 1.400e-01\n",
      "Epoch 11/100 | Batch 71/88 | train_accuracy: 9.472e-01, train_loss: 1.401e-01\n",
      "Epoch 11/100 | Batch 72/88 | train_accuracy: 9.453e-01, train_loss: 1.422e-01\n",
      "Epoch 11/100 | Batch 73/88 | train_accuracy: 9.461e-01, train_loss: 1.410e-01\n",
      "Epoch 11/100 | Batch 74/88 | train_accuracy: 9.459e-01, train_loss: 1.400e-01\n",
      "Epoch 11/100 | Batch 75/88 | train_accuracy: 9.458e-01, train_loss: 1.392e-01\n",
      "Epoch 11/100 | Batch 76/88 | train_accuracy: 9.457e-01, train_loss: 1.400e-01\n",
      "Epoch 11/100 | Batch 77/88 | train_accuracy: 9.464e-01, train_loss: 1.391e-01\n",
      "Epoch 11/100 | Batch 78/88 | train_accuracy: 9.471e-01, train_loss: 1.384e-01\n",
      "Epoch 11/100 | Batch 79/88 | train_accuracy: 9.470e-01, train_loss: 1.394e-01\n",
      "Epoch 11/100 | Batch 80/88 | train_accuracy: 9.469e-01, train_loss: 1.401e-01\n",
      "Epoch 11/100 | Batch 81/88 | train_accuracy: 9.475e-01, train_loss: 1.399e-01\n",
      "Epoch 11/100 | Batch 82/88 | train_accuracy: 9.474e-01, train_loss: 1.398e-01\n",
      "Epoch 11/100 | Batch 83/88 | train_accuracy: 9.473e-01, train_loss: 1.396e-01\n",
      "Epoch 11/100 | Batch 84/88 | train_accuracy: 9.472e-01, train_loss: 1.398e-01\n",
      "Epoch 11/100 | Batch 85/88 | train_accuracy: 9.478e-01, train_loss: 1.390e-01\n",
      "Epoch 11/100 | Batch 86/88 | train_accuracy: 9.469e-01, train_loss: 1.407e-01\n",
      "Epoch 11/100 | Batch 87/88 | train_accuracy: 9.476e-01, train_loss: 1.400e-01\n",
      "Epoch 11/100 | Batch 88/88 | train_accuracy: 9.464e-01, train_loss: 1.415e-01\n",
      "Epoch 11/100 | val_accuracy: 7.100e-01, val_loss: 7.954e-01\n",
      "====================\n",
      "Epoch 12/100 | Batch 1/88 | train_accuracy: 1.000e+00, train_loss: 4.442e-02\n",
      "Epoch 12/100 | Batch 2/88 | train_accuracy: 1.000e+00, train_loss: 4.386e-02\n",
      "Epoch 12/100 | Batch 3/88 | train_accuracy: 1.000e+00, train_loss: 3.433e-02\n",
      "Epoch 12/100 | Batch 4/88 | train_accuracy: 1.000e+00, train_loss: 4.514e-02\n",
      "Epoch 12/100 | Batch 5/88 | train_accuracy: 9.750e-01, train_loss: 7.304e-02\n",
      "Epoch 12/100 | Batch 6/88 | train_accuracy: 9.792e-01, train_loss: 6.657e-02\n",
      "Epoch 12/100 | Batch 7/88 | train_accuracy: 9.732e-01, train_loss: 7.024e-02\n",
      "Epoch 12/100 | Batch 8/88 | train_accuracy: 9.766e-01, train_loss: 6.637e-02\n",
      "Epoch 12/100 | Batch 9/88 | train_accuracy: 9.722e-01, train_loss: 7.727e-02\n",
      "Epoch 12/100 | Batch 10/88 | train_accuracy: 9.688e-01, train_loss: 8.230e-02\n",
      "Epoch 12/100 | Batch 11/88 | train_accuracy: 9.659e-01, train_loss: 8.344e-02\n",
      "Epoch 12/100 | Batch 12/88 | train_accuracy: 9.688e-01, train_loss: 7.770e-02\n",
      "Epoch 12/100 | Batch 13/88 | train_accuracy: 9.712e-01, train_loss: 7.443e-02\n",
      "Epoch 12/100 | Batch 14/88 | train_accuracy: 9.732e-01, train_loss: 7.411e-02\n",
      "Epoch 12/100 | Batch 15/88 | train_accuracy: 9.750e-01, train_loss: 7.096e-02\n",
      "Epoch 12/100 | Batch 16/88 | train_accuracy: 9.766e-01, train_loss: 6.961e-02\n",
      "Epoch 12/100 | Batch 17/88 | train_accuracy: 9.779e-01, train_loss: 6.709e-02\n",
      "Epoch 12/100 | Batch 18/88 | train_accuracy: 9.792e-01, train_loss: 6.558e-02\n",
      "Epoch 12/100 | Batch 19/88 | train_accuracy: 9.803e-01, train_loss: 6.332e-02\n",
      "Epoch 12/100 | Batch 20/88 | train_accuracy: 9.781e-01, train_loss: 6.574e-02\n",
      "Epoch 12/100 | Batch 21/88 | train_accuracy: 9.792e-01, train_loss: 6.498e-02\n",
      "Epoch 12/100 | Batch 22/88 | train_accuracy: 9.801e-01, train_loss: 6.315e-02\n",
      "Epoch 12/100 | Batch 23/88 | train_accuracy: 9.810e-01, train_loss: 6.059e-02\n",
      "Epoch 12/100 | Batch 24/88 | train_accuracy: 9.818e-01, train_loss: 5.872e-02\n",
      "Epoch 12/100 | Batch 25/88 | train_accuracy: 9.825e-01, train_loss: 5.683e-02\n",
      "Epoch 12/100 | Batch 26/88 | train_accuracy: 9.808e-01, train_loss: 5.892e-02\n",
      "Epoch 12/100 | Batch 27/88 | train_accuracy: 9.815e-01, train_loss: 5.741e-02\n",
      "Epoch 12/100 | Batch 28/88 | train_accuracy: 9.821e-01, train_loss: 5.776e-02\n",
      "Epoch 12/100 | Batch 29/88 | train_accuracy: 9.806e-01, train_loss: 6.012e-02\n",
      "Epoch 12/100 | Batch 30/88 | train_accuracy: 9.792e-01, train_loss: 6.266e-02\n",
      "Epoch 12/100 | Batch 31/88 | train_accuracy: 9.798e-01, train_loss: 6.180e-02\n",
      "Epoch 12/100 | Batch 32/88 | train_accuracy: 9.805e-01, train_loss: 6.080e-02\n",
      "Epoch 12/100 | Batch 33/88 | train_accuracy: 9.792e-01, train_loss: 6.345e-02\n",
      "Epoch 12/100 | Batch 34/88 | train_accuracy: 9.798e-01, train_loss: 6.200e-02\n",
      "Epoch 12/100 | Batch 35/88 | train_accuracy: 9.786e-01, train_loss: 6.236e-02\n",
      "Epoch 12/100 | Batch 36/88 | train_accuracy: 9.792e-01, train_loss: 6.087e-02\n",
      "Epoch 12/100 | Batch 37/88 | train_accuracy: 9.797e-01, train_loss: 5.997e-02\n",
      "Epoch 12/100 | Batch 38/88 | train_accuracy: 9.803e-01, train_loss: 5.892e-02\n",
      "Epoch 12/100 | Batch 39/88 | train_accuracy: 9.808e-01, train_loss: 5.788e-02\n",
      "Epoch 12/100 | Batch 40/88 | train_accuracy: 9.812e-01, train_loss: 5.772e-02\n",
      "Epoch 12/100 | Batch 41/88 | train_accuracy: 9.817e-01, train_loss: 5.668e-02\n",
      "Epoch 12/100 | Batch 42/88 | train_accuracy: 9.807e-01, train_loss: 6.112e-02\n",
      "Epoch 12/100 | Batch 43/88 | train_accuracy: 9.811e-01, train_loss: 6.114e-02\n",
      "Epoch 12/100 | Batch 44/88 | train_accuracy: 9.815e-01, train_loss: 5.992e-02\n",
      "Epoch 12/100 | Batch 45/88 | train_accuracy: 9.819e-01, train_loss: 6.026e-02\n",
      "Epoch 12/100 | Batch 46/88 | train_accuracy: 9.823e-01, train_loss: 5.931e-02\n",
      "Epoch 12/100 | Batch 47/88 | train_accuracy: 9.827e-01, train_loss: 5.828e-02\n",
      "Epoch 12/100 | Batch 48/88 | train_accuracy: 9.818e-01, train_loss: 5.872e-02\n",
      "Epoch 12/100 | Batch 49/88 | train_accuracy: 9.821e-01, train_loss: 5.805e-02\n",
      "Epoch 12/100 | Batch 50/88 | train_accuracy: 9.788e-01, train_loss: 6.420e-02\n",
      "Epoch 12/100 | Batch 51/88 | train_accuracy: 9.779e-01, train_loss: 6.800e-02\n",
      "Epoch 12/100 | Batch 52/88 | train_accuracy: 9.772e-01, train_loss: 6.891e-02\n",
      "Epoch 12/100 | Batch 53/88 | train_accuracy: 9.752e-01, train_loss: 7.171e-02\n",
      "Epoch 12/100 | Batch 54/88 | train_accuracy: 9.722e-01, train_loss: 7.737e-02\n",
      "Epoch 12/100 | Batch 55/88 | train_accuracy: 9.716e-01, train_loss: 7.758e-02\n",
      "Epoch 12/100 | Batch 56/88 | train_accuracy: 9.721e-01, train_loss: 7.707e-02\n",
      "Epoch 12/100 | Batch 57/88 | train_accuracy: 9.726e-01, train_loss: 7.639e-02\n",
      "Epoch 12/100 | Batch 58/88 | train_accuracy: 9.709e-01, train_loss: 7.917e-02\n",
      "Epoch 12/100 | Batch 59/88 | train_accuracy: 9.703e-01, train_loss: 7.951e-02\n",
      "Epoch 12/100 | Batch 60/88 | train_accuracy: 9.688e-01, train_loss: 8.137e-02\n",
      "Epoch 12/100 | Batch 61/88 | train_accuracy: 9.693e-01, train_loss: 8.053e-02\n",
      "Epoch 12/100 | Batch 62/88 | train_accuracy: 9.657e-01, train_loss: 8.988e-02\n",
      "Epoch 12/100 | Batch 63/88 | train_accuracy: 9.633e-01, train_loss: 9.336e-02\n",
      "Epoch 12/100 | Batch 64/88 | train_accuracy: 9.619e-01, train_loss: 9.505e-02\n",
      "Epoch 12/100 | Batch 65/88 | train_accuracy: 9.606e-01, train_loss: 9.800e-02\n",
      "Epoch 12/100 | Batch 66/88 | train_accuracy: 9.602e-01, train_loss: 9.790e-02\n",
      "Epoch 12/100 | Batch 67/88 | train_accuracy: 9.590e-01, train_loss: 1.020e-01\n",
      "Epoch 12/100 | Batch 68/88 | train_accuracy: 9.596e-01, train_loss: 1.017e-01\n",
      "Epoch 12/100 | Batch 69/88 | train_accuracy: 9.574e-01, train_loss: 1.038e-01\n",
      "Epoch 12/100 | Batch 70/88 | train_accuracy: 9.580e-01, train_loss: 1.039e-01\n",
      "Epoch 12/100 | Batch 71/88 | train_accuracy: 9.569e-01, train_loss: 1.056e-01\n",
      "Epoch 12/100 | Batch 72/88 | train_accuracy: 9.566e-01, train_loss: 1.069e-01\n",
      "Epoch 12/100 | Batch 73/88 | train_accuracy: 9.563e-01, train_loss: 1.074e-01\n",
      "Epoch 12/100 | Batch 74/88 | train_accuracy: 9.535e-01, train_loss: 1.114e-01\n",
      "Epoch 12/100 | Batch 75/88 | train_accuracy: 9.525e-01, train_loss: 1.127e-01\n",
      "Epoch 12/100 | Batch 76/88 | train_accuracy: 9.515e-01, train_loss: 1.141e-01\n",
      "Epoch 12/100 | Batch 77/88 | train_accuracy: 9.513e-01, train_loss: 1.156e-01\n",
      "Epoch 12/100 | Batch 78/88 | train_accuracy: 9.519e-01, train_loss: 1.149e-01\n",
      "Epoch 12/100 | Batch 79/88 | train_accuracy: 9.509e-01, train_loss: 1.179e-01\n",
      "Epoch 12/100 | Batch 80/88 | train_accuracy: 9.500e-01, train_loss: 1.195e-01\n",
      "Epoch 12/100 | Batch 81/88 | train_accuracy: 9.483e-01, train_loss: 1.208e-01\n",
      "Epoch 12/100 | Batch 82/88 | train_accuracy: 9.489e-01, train_loss: 1.217e-01\n",
      "Epoch 12/100 | Batch 83/88 | train_accuracy: 9.495e-01, train_loss: 1.209e-01\n",
      "Epoch 12/100 | Batch 84/88 | train_accuracy: 9.494e-01, train_loss: 1.212e-01\n",
      "Epoch 12/100 | Batch 85/88 | train_accuracy: 9.485e-01, train_loss: 1.219e-01\n",
      "Epoch 12/100 | Batch 86/88 | train_accuracy: 9.491e-01, train_loss: 1.213e-01\n",
      "Epoch 12/100 | Batch 87/88 | train_accuracy: 9.483e-01, train_loss: 1.220e-01\n",
      "Epoch 12/100 | Batch 88/88 | train_accuracy: 9.486e-01, train_loss: 1.210e-01\n",
      "Epoch 12/100 | val_accuracy: 7.250e-01, val_loss: 7.029e-01\n",
      "====================\n",
      "Epoch 13/100 | Batch 1/88 | train_accuracy: 9.375e-01, train_loss: 1.670e-01\n",
      "Epoch 13/100 | Batch 2/88 | train_accuracy: 9.375e-01, train_loss: 1.690e-01\n",
      "Epoch 13/100 | Batch 3/88 | train_accuracy: 9.583e-01, train_loss: 1.346e-01\n",
      "Epoch 13/100 | Batch 4/88 | train_accuracy: 9.531e-01, train_loss: 1.247e-01\n",
      "Epoch 13/100 | Batch 5/88 | train_accuracy: 9.625e-01, train_loss: 1.019e-01\n",
      "Epoch 13/100 | Batch 6/88 | train_accuracy: 9.583e-01, train_loss: 1.036e-01\n",
      "Epoch 13/100 | Batch 7/88 | train_accuracy: 9.643e-01, train_loss: 9.982e-02\n",
      "Epoch 13/100 | Batch 8/88 | train_accuracy: 9.609e-01, train_loss: 1.174e-01\n",
      "Epoch 13/100 | Batch 9/88 | train_accuracy: 9.653e-01, train_loss: 1.076e-01\n",
      "Epoch 13/100 | Batch 10/88 | train_accuracy: 9.563e-01, train_loss: 1.155e-01\n",
      "Epoch 13/100 | Batch 11/88 | train_accuracy: 9.602e-01, train_loss: 1.063e-01\n",
      "Epoch 13/100 | Batch 12/88 | train_accuracy: 9.583e-01, train_loss: 1.060e-01\n",
      "Epoch 13/100 | Batch 13/88 | train_accuracy: 9.615e-01, train_loss: 9.965e-02\n",
      "Epoch 13/100 | Batch 14/88 | train_accuracy: 9.598e-01, train_loss: 9.723e-02\n",
      "Epoch 13/100 | Batch 15/88 | train_accuracy: 9.625e-01, train_loss: 9.126e-02\n",
      "Epoch 13/100 | Batch 16/88 | train_accuracy: 9.609e-01, train_loss: 9.205e-02\n",
      "Epoch 13/100 | Batch 17/88 | train_accuracy: 9.632e-01, train_loss: 8.808e-02\n",
      "Epoch 13/100 | Batch 18/88 | train_accuracy: 9.653e-01, train_loss: 8.598e-02\n",
      "Epoch 13/100 | Batch 19/88 | train_accuracy: 9.638e-01, train_loss: 8.793e-02\n",
      "Epoch 13/100 | Batch 20/88 | train_accuracy: 9.656e-01, train_loss: 8.408e-02\n",
      "Epoch 13/100 | Batch 21/88 | train_accuracy: 9.673e-01, train_loss: 8.476e-02\n",
      "Epoch 13/100 | Batch 22/88 | train_accuracy: 9.659e-01, train_loss: 8.528e-02\n",
      "Epoch 13/100 | Batch 23/88 | train_accuracy: 9.647e-01, train_loss: 8.836e-02\n",
      "Epoch 13/100 | Batch 24/88 | train_accuracy: 9.661e-01, train_loss: 8.548e-02\n",
      "Epoch 13/100 | Batch 25/88 | train_accuracy: 9.650e-01, train_loss: 9.358e-02\n",
      "Epoch 13/100 | Batch 26/88 | train_accuracy: 9.663e-01, train_loss: 9.192e-02\n",
      "Epoch 13/100 | Batch 27/88 | train_accuracy: 9.676e-01, train_loss: 8.904e-02\n",
      "Epoch 13/100 | Batch 28/88 | train_accuracy: 9.688e-01, train_loss: 8.748e-02\n",
      "Epoch 13/100 | Batch 29/88 | train_accuracy: 9.677e-01, train_loss: 8.872e-02\n",
      "Epoch 13/100 | Batch 30/88 | train_accuracy: 9.688e-01, train_loss: 8.715e-02\n",
      "Epoch 13/100 | Batch 31/88 | train_accuracy: 9.698e-01, train_loss: 8.464e-02\n",
      "Epoch 13/100 | Batch 32/88 | train_accuracy: 9.707e-01, train_loss: 8.298e-02\n",
      "Epoch 13/100 | Batch 33/88 | train_accuracy: 9.716e-01, train_loss: 8.067e-02\n",
      "Epoch 13/100 | Batch 34/88 | train_accuracy: 9.706e-01, train_loss: 8.073e-02\n",
      "Epoch 13/100 | Batch 35/88 | train_accuracy: 9.714e-01, train_loss: 7.876e-02\n",
      "Epoch 13/100 | Batch 36/88 | train_accuracy: 9.705e-01, train_loss: 8.197e-02\n",
      "Epoch 13/100 | Batch 37/88 | train_accuracy: 9.713e-01, train_loss: 8.117e-02\n",
      "Epoch 13/100 | Batch 38/88 | train_accuracy: 9.704e-01, train_loss: 8.090e-02\n",
      "Epoch 13/100 | Batch 39/88 | train_accuracy: 9.712e-01, train_loss: 7.893e-02\n",
      "Epoch 13/100 | Batch 40/88 | train_accuracy: 9.719e-01, train_loss: 7.705e-02\n",
      "Epoch 13/100 | Batch 41/88 | train_accuracy: 9.726e-01, train_loss: 7.707e-02\n",
      "Epoch 13/100 | Batch 42/88 | train_accuracy: 9.702e-01, train_loss: 8.126e-02\n",
      "Epoch 13/100 | Batch 43/88 | train_accuracy: 9.709e-01, train_loss: 7.991e-02\n",
      "Epoch 13/100 | Batch 44/88 | train_accuracy: 9.702e-01, train_loss: 8.242e-02\n",
      "Epoch 13/100 | Batch 45/88 | train_accuracy: 9.708e-01, train_loss: 8.155e-02\n",
      "Epoch 13/100 | Batch 46/88 | train_accuracy: 9.688e-01, train_loss: 8.464e-02\n",
      "Epoch 13/100 | Batch 47/88 | train_accuracy: 9.694e-01, train_loss: 8.451e-02\n",
      "Epoch 13/100 | Batch 48/88 | train_accuracy: 9.688e-01, train_loss: 8.418e-02\n",
      "Epoch 13/100 | Batch 49/88 | train_accuracy: 9.694e-01, train_loss: 8.369e-02\n",
      "Epoch 13/100 | Batch 50/88 | train_accuracy: 9.675e-01, train_loss: 8.760e-02\n",
      "Epoch 13/100 | Batch 51/88 | train_accuracy: 9.669e-01, train_loss: 8.807e-02\n",
      "Epoch 13/100 | Batch 52/88 | train_accuracy: 9.675e-01, train_loss: 8.687e-02\n",
      "Epoch 13/100 | Batch 53/88 | train_accuracy: 9.670e-01, train_loss: 9.189e-02\n",
      "Epoch 13/100 | Batch 54/88 | train_accuracy: 9.653e-01, train_loss: 9.409e-02\n",
      "Epoch 13/100 | Batch 55/88 | train_accuracy: 9.659e-01, train_loss: 9.296e-02\n",
      "Epoch 13/100 | Batch 56/88 | train_accuracy: 9.654e-01, train_loss: 9.436e-02\n",
      "Epoch 13/100 | Batch 57/88 | train_accuracy: 9.649e-01, train_loss: 9.546e-02\n",
      "Epoch 13/100 | Batch 58/88 | train_accuracy: 9.644e-01, train_loss: 9.665e-02\n",
      "Epoch 13/100 | Batch 59/88 | train_accuracy: 9.650e-01, train_loss: 9.579e-02\n",
      "Epoch 13/100 | Batch 60/88 | train_accuracy: 9.656e-01, train_loss: 9.455e-02\n",
      "Epoch 13/100 | Batch 61/88 | train_accuracy: 9.631e-01, train_loss: 9.865e-02\n",
      "Epoch 13/100 | Batch 62/88 | train_accuracy: 9.637e-01, train_loss: 9.744e-02\n",
      "Epoch 13/100 | Batch 63/88 | train_accuracy: 9.643e-01, train_loss: 9.702e-02\n",
      "Epoch 13/100 | Batch 64/88 | train_accuracy: 9.639e-01, train_loss: 9.709e-02\n",
      "Epoch 13/100 | Batch 65/88 | train_accuracy: 9.644e-01, train_loss: 9.664e-02\n",
      "Epoch 13/100 | Batch 66/88 | train_accuracy: 9.650e-01, train_loss: 9.578e-02\n",
      "Epoch 13/100 | Batch 67/88 | train_accuracy: 9.646e-01, train_loss: 9.724e-02\n",
      "Epoch 13/100 | Batch 68/88 | train_accuracy: 9.642e-01, train_loss: 9.685e-02\n",
      "Epoch 13/100 | Batch 69/88 | train_accuracy: 9.647e-01, train_loss: 9.651e-02\n",
      "Epoch 13/100 | Batch 70/88 | train_accuracy: 9.652e-01, train_loss: 9.551e-02\n",
      "Epoch 13/100 | Batch 71/88 | train_accuracy: 9.657e-01, train_loss: 9.457e-02\n",
      "Epoch 13/100 | Batch 72/88 | train_accuracy: 9.661e-01, train_loss: 9.390e-02\n",
      "Epoch 13/100 | Batch 73/88 | train_accuracy: 9.658e-01, train_loss: 9.360e-02\n",
      "Epoch 13/100 | Batch 74/88 | train_accuracy: 9.662e-01, train_loss: 9.299e-02\n",
      "Epoch 13/100 | Batch 75/88 | train_accuracy: 9.667e-01, train_loss: 9.260e-02\n",
      "Epoch 13/100 | Batch 76/88 | train_accuracy: 9.671e-01, train_loss: 9.250e-02\n",
      "Epoch 13/100 | Batch 77/88 | train_accuracy: 9.675e-01, train_loss: 9.179e-02\n",
      "Epoch 13/100 | Batch 78/88 | train_accuracy: 9.671e-01, train_loss: 9.140e-02\n",
      "Epoch 13/100 | Batch 79/88 | train_accuracy: 9.676e-01, train_loss: 9.138e-02\n",
      "Epoch 13/100 | Batch 80/88 | train_accuracy: 9.680e-01, train_loss: 9.050e-02\n",
      "Epoch 13/100 | Batch 81/88 | train_accuracy: 9.684e-01, train_loss: 8.966e-02\n",
      "Epoch 13/100 | Batch 82/88 | train_accuracy: 9.688e-01, train_loss: 8.886e-02\n",
      "Epoch 13/100 | Batch 83/88 | train_accuracy: 9.684e-01, train_loss: 8.917e-02\n",
      "Epoch 13/100 | Batch 84/88 | train_accuracy: 9.680e-01, train_loss: 9.047e-02\n",
      "Epoch 13/100 | Batch 85/88 | train_accuracy: 9.684e-01, train_loss: 9.028e-02\n",
      "Epoch 13/100 | Batch 86/88 | train_accuracy: 9.688e-01, train_loss: 8.981e-02\n",
      "Epoch 13/100 | Batch 87/88 | train_accuracy: 9.677e-01, train_loss: 9.171e-02\n",
      "Epoch 13/100 | Batch 88/88 | train_accuracy: 9.679e-01, train_loss: 9.097e-02\n",
      "Epoch 13/100 | val_accuracy: 6.850e-01, val_loss: 1.032e+00\n",
      "====================\n",
      "Epoch 14/100 | Batch 1/88 | train_accuracy: 1.000e+00, train_loss: 2.390e-02\n",
      "Epoch 14/100 | Batch 2/88 | train_accuracy: 9.375e-01, train_loss: 1.802e-01\n",
      "Epoch 14/100 | Batch 3/88 | train_accuracy: 9.583e-01, train_loss: 1.294e-01\n",
      "Epoch 14/100 | Batch 4/88 | train_accuracy: 9.688e-01, train_loss: 1.115e-01\n",
      "Epoch 14/100 | Batch 5/88 | train_accuracy: 9.750e-01, train_loss: 9.465e-02\n",
      "Epoch 14/100 | Batch 6/88 | train_accuracy: 9.688e-01, train_loss: 1.117e-01\n",
      "Epoch 14/100 | Batch 7/88 | train_accuracy: 9.464e-01, train_loss: 2.031e-01\n",
      "Epoch 14/100 | Batch 8/88 | train_accuracy: 9.531e-01, train_loss: 1.799e-01\n",
      "Epoch 14/100 | Batch 9/88 | train_accuracy: 9.583e-01, train_loss: 1.635e-01\n",
      "Epoch 14/100 | Batch 10/88 | train_accuracy: 9.563e-01, train_loss: 1.595e-01\n",
      "Epoch 14/100 | Batch 11/88 | train_accuracy: 9.489e-01, train_loss: 1.597e-01\n",
      "Epoch 14/100 | Batch 12/88 | train_accuracy: 9.531e-01, train_loss: 1.499e-01\n",
      "Epoch 14/100 | Batch 13/88 | train_accuracy: 9.567e-01, train_loss: 1.406e-01\n",
      "Epoch 14/100 | Batch 14/88 | train_accuracy: 9.598e-01, train_loss: 1.316e-01\n",
      "Epoch 14/100 | Batch 15/88 | train_accuracy: 9.625e-01, train_loss: 1.230e-01\n",
      "Epoch 14/100 | Batch 16/88 | train_accuracy: 9.648e-01, train_loss: 1.162e-01\n",
      "Epoch 14/100 | Batch 17/88 | train_accuracy: 9.669e-01, train_loss: 1.108e-01\n",
      "Epoch 14/100 | Batch 18/88 | train_accuracy: 9.688e-01, train_loss: 1.065e-01\n",
      "Epoch 14/100 | Batch 19/88 | train_accuracy: 9.704e-01, train_loss: 1.036e-01\n",
      "Epoch 14/100 | Batch 20/88 | train_accuracy: 9.719e-01, train_loss: 1.004e-01\n",
      "Epoch 14/100 | Batch 21/88 | train_accuracy: 9.732e-01, train_loss: 9.630e-02\n",
      "Epoch 14/100 | Batch 22/88 | train_accuracy: 9.744e-01, train_loss: 9.314e-02\n",
      "Epoch 14/100 | Batch 23/88 | train_accuracy: 9.755e-01, train_loss: 8.993e-02\n",
      "Epoch 14/100 | Batch 24/88 | train_accuracy: 9.766e-01, train_loss: 8.910e-02\n",
      "Epoch 14/100 | Batch 25/88 | train_accuracy: 9.750e-01, train_loss: 9.181e-02\n",
      "Epoch 14/100 | Batch 26/88 | train_accuracy: 9.760e-01, train_loss: 9.048e-02\n",
      "Epoch 14/100 | Batch 27/88 | train_accuracy: 9.745e-01, train_loss: 9.127e-02\n",
      "Epoch 14/100 | Batch 28/88 | train_accuracy: 9.754e-01, train_loss: 8.907e-02\n",
      "Epoch 14/100 | Batch 29/88 | train_accuracy: 9.763e-01, train_loss: 8.648e-02\n",
      "Epoch 14/100 | Batch 30/88 | train_accuracy: 9.771e-01, train_loss: 8.509e-02\n",
      "Epoch 14/100 | Batch 31/88 | train_accuracy: 9.778e-01, train_loss: 8.332e-02\n",
      "Epoch 14/100 | Batch 32/88 | train_accuracy: 9.785e-01, train_loss: 8.250e-02\n",
      "Epoch 14/100 | Batch 33/88 | train_accuracy: 9.792e-01, train_loss: 8.020e-02\n",
      "Epoch 14/100 | Batch 34/88 | train_accuracy: 9.779e-01, train_loss: 8.009e-02\n",
      "Epoch 14/100 | Batch 35/88 | train_accuracy: 9.768e-01, train_loss: 8.021e-02\n",
      "Epoch 14/100 | Batch 36/88 | train_accuracy: 9.774e-01, train_loss: 7.854e-02\n",
      "Epoch 14/100 | Batch 37/88 | train_accuracy: 9.780e-01, train_loss: 7.718e-02\n",
      "Epoch 14/100 | Batch 38/88 | train_accuracy: 9.786e-01, train_loss: 7.540e-02\n",
      "Epoch 14/100 | Batch 39/88 | train_accuracy: 9.792e-01, train_loss: 7.453e-02\n",
      "Epoch 14/100 | Batch 40/88 | train_accuracy: 9.781e-01, train_loss: 7.408e-02\n",
      "Epoch 14/100 | Batch 41/88 | train_accuracy: 9.787e-01, train_loss: 7.255e-02\n",
      "Epoch 14/100 | Batch 42/88 | train_accuracy: 9.792e-01, train_loss: 7.088e-02\n",
      "Epoch 14/100 | Batch 43/88 | train_accuracy: 9.797e-01, train_loss: 6.945e-02\n",
      "Epoch 14/100 | Batch 44/88 | train_accuracy: 9.801e-01, train_loss: 6.799e-02\n",
      "Epoch 14/100 | Batch 45/88 | train_accuracy: 9.806e-01, train_loss: 6.684e-02\n",
      "Epoch 14/100 | Batch 46/88 | train_accuracy: 9.810e-01, train_loss: 6.549e-02\n",
      "Epoch 14/100 | Batch 47/88 | train_accuracy: 9.814e-01, train_loss: 6.484e-02\n",
      "Epoch 14/100 | Batch 48/88 | train_accuracy: 9.818e-01, train_loss: 6.363e-02\n",
      "Epoch 14/100 | Batch 49/88 | train_accuracy: 9.821e-01, train_loss: 6.256e-02\n",
      "Epoch 14/100 | Batch 50/88 | train_accuracy: 9.812e-01, train_loss: 6.229e-02\n",
      "Epoch 14/100 | Batch 51/88 | train_accuracy: 9.804e-01, train_loss: 6.415e-02\n",
      "Epoch 14/100 | Batch 52/88 | train_accuracy: 9.808e-01, train_loss: 6.294e-02\n",
      "Epoch 14/100 | Batch 53/88 | train_accuracy: 9.811e-01, train_loss: 6.227e-02\n",
      "Epoch 14/100 | Batch 54/88 | train_accuracy: 9.815e-01, train_loss: 6.182e-02\n",
      "Epoch 14/100 | Batch 55/88 | train_accuracy: 9.807e-01, train_loss: 6.206e-02\n",
      "Epoch 14/100 | Batch 56/88 | train_accuracy: 9.799e-01, train_loss: 6.305e-02\n",
      "Epoch 14/100 | Batch 57/88 | train_accuracy: 9.792e-01, train_loss: 6.301e-02\n",
      "Epoch 14/100 | Batch 58/88 | train_accuracy: 9.795e-01, train_loss: 6.209e-02\n",
      "Epoch 14/100 | Batch 59/88 | train_accuracy: 9.799e-01, train_loss: 6.128e-02\n",
      "Epoch 14/100 | Batch 60/88 | train_accuracy: 9.802e-01, train_loss: 6.046e-02\n",
      "Epoch 14/100 | Batch 61/88 | train_accuracy: 9.795e-01, train_loss: 6.172e-02\n",
      "Epoch 14/100 | Batch 62/88 | train_accuracy: 9.798e-01, train_loss: 6.146e-02\n",
      "Epoch 14/100 | Batch 63/88 | train_accuracy: 9.782e-01, train_loss: 6.261e-02\n",
      "Epoch 14/100 | Batch 64/88 | train_accuracy: 9.775e-01, train_loss: 6.600e-02\n",
      "Epoch 14/100 | Batch 65/88 | train_accuracy: 9.769e-01, train_loss: 6.807e-02\n",
      "Epoch 14/100 | Batch 66/88 | train_accuracy: 9.773e-01, train_loss: 6.755e-02\n",
      "Epoch 14/100 | Batch 67/88 | train_accuracy: 9.776e-01, train_loss: 6.665e-02\n",
      "Epoch 14/100 | Batch 68/88 | train_accuracy: 9.779e-01, train_loss: 6.641e-02\n",
      "Epoch 14/100 | Batch 69/88 | train_accuracy: 9.764e-01, train_loss: 6.904e-02\n",
      "Epoch 14/100 | Batch 70/88 | train_accuracy: 9.768e-01, train_loss: 6.836e-02\n",
      "Epoch 14/100 | Batch 71/88 | train_accuracy: 9.771e-01, train_loss: 6.767e-02\n",
      "Epoch 14/100 | Batch 72/88 | train_accuracy: 9.774e-01, train_loss: 6.689e-02\n",
      "Epoch 14/100 | Batch 73/88 | train_accuracy: 9.777e-01, train_loss: 6.708e-02\n",
      "Epoch 14/100 | Batch 74/88 | train_accuracy: 9.780e-01, train_loss: 6.623e-02\n",
      "Epoch 14/100 | Batch 75/88 | train_accuracy: 9.783e-01, train_loss: 6.552e-02\n",
      "Epoch 14/100 | Batch 76/88 | train_accuracy: 9.786e-01, train_loss: 6.486e-02\n",
      "Epoch 14/100 | Batch 77/88 | train_accuracy: 9.789e-01, train_loss: 6.478e-02\n",
      "Epoch 14/100 | Batch 78/88 | train_accuracy: 9.776e-01, train_loss: 6.803e-02\n",
      "Epoch 14/100 | Batch 79/88 | train_accuracy: 9.778e-01, train_loss: 6.721e-02\n",
      "Epoch 14/100 | Batch 80/88 | train_accuracy: 9.781e-01, train_loss: 6.743e-02\n",
      "Epoch 14/100 | Batch 81/88 | train_accuracy: 9.776e-01, train_loss: 7.068e-02\n",
      "Epoch 14/100 | Batch 82/88 | train_accuracy: 9.771e-01, train_loss: 7.146e-02\n",
      "Epoch 14/100 | Batch 83/88 | train_accuracy: 9.767e-01, train_loss: 7.786e-02\n",
      "Epoch 14/100 | Batch 84/88 | train_accuracy: 9.762e-01, train_loss: 7.828e-02\n",
      "Epoch 14/100 | Batch 85/88 | train_accuracy: 9.757e-01, train_loss: 7.883e-02\n",
      "Epoch 14/100 | Batch 86/88 | train_accuracy: 9.760e-01, train_loss: 7.831e-02\n",
      "Epoch 14/100 | Batch 87/88 | train_accuracy: 9.763e-01, train_loss: 7.777e-02\n",
      "Epoch 14/100 | Batch 88/88 | train_accuracy: 9.764e-01, train_loss: 7.795e-02\n",
      "Epoch 14/100 | val_accuracy: 6.850e-01, val_loss: 9.888e-01\n",
      "====================\n",
      "Epoch 15/100 | Batch 1/88 | train_accuracy: 1.000e+00, train_loss: 1.985e-02\n",
      "Epoch 15/100 | Batch 2/88 | train_accuracy: 1.000e+00, train_loss: 4.273e-02\n",
      "Epoch 15/100 | Batch 3/88 | train_accuracy: 1.000e+00, train_loss: 3.305e-02\n",
      "Epoch 15/100 | Batch 4/88 | train_accuracy: 1.000e+00, train_loss: 2.623e-02\n",
      "Epoch 15/100 | Batch 5/88 | train_accuracy: 1.000e+00, train_loss: 2.761e-02\n",
      "Epoch 15/100 | Batch 6/88 | train_accuracy: 1.000e+00, train_loss: 2.977e-02\n",
      "Epoch 15/100 | Batch 7/88 | train_accuracy: 9.911e-01, train_loss: 8.819e-02\n",
      "Epoch 15/100 | Batch 8/88 | train_accuracy: 9.922e-01, train_loss: 8.165e-02\n",
      "Epoch 15/100 | Batch 9/88 | train_accuracy: 9.931e-01, train_loss: 7.745e-02\n",
      "Epoch 15/100 | Batch 10/88 | train_accuracy: 9.938e-01, train_loss: 7.121e-02\n",
      "Epoch 15/100 | Batch 11/88 | train_accuracy: 9.943e-01, train_loss: 6.747e-02\n",
      "Epoch 15/100 | Batch 12/88 | train_accuracy: 9.948e-01, train_loss: 6.332e-02\n",
      "Epoch 15/100 | Batch 13/88 | train_accuracy: 9.952e-01, train_loss: 6.019e-02\n",
      "Epoch 15/100 | Batch 14/88 | train_accuracy: 9.955e-01, train_loss: 5.722e-02\n",
      "Epoch 15/100 | Batch 15/88 | train_accuracy: 9.958e-01, train_loss: 5.380e-02\n",
      "Epoch 15/100 | Batch 16/88 | train_accuracy: 9.922e-01, train_loss: 6.459e-02\n",
      "Epoch 15/100 | Batch 17/88 | train_accuracy: 9.926e-01, train_loss: 6.119e-02\n",
      "Epoch 15/100 | Batch 18/88 | train_accuracy: 9.931e-01, train_loss: 5.954e-02\n",
      "Epoch 15/100 | Batch 19/88 | train_accuracy: 9.934e-01, train_loss: 5.756e-02\n",
      "Epoch 15/100 | Batch 20/88 | train_accuracy: 9.938e-01, train_loss: 5.832e-02\n",
      "Epoch 15/100 | Batch 21/88 | train_accuracy: 9.940e-01, train_loss: 5.627e-02\n",
      "Epoch 15/100 | Batch 22/88 | train_accuracy: 9.943e-01, train_loss: 5.723e-02\n",
      "Epoch 15/100 | Batch 23/88 | train_accuracy: 9.946e-01, train_loss: 5.744e-02\n",
      "Epoch 15/100 | Batch 24/88 | train_accuracy: 9.948e-01, train_loss: 5.521e-02\n",
      "Epoch 15/100 | Batch 25/88 | train_accuracy: 9.950e-01, train_loss: 5.520e-02\n",
      "Epoch 15/100 | Batch 26/88 | train_accuracy: 9.952e-01, train_loss: 5.341e-02\n",
      "Epoch 15/100 | Batch 27/88 | train_accuracy: 9.954e-01, train_loss: 5.211e-02\n",
      "Epoch 15/100 | Batch 28/88 | train_accuracy: 9.955e-01, train_loss: 5.168e-02\n",
      "Epoch 15/100 | Batch 29/88 | train_accuracy: 9.935e-01, train_loss: 5.521e-02\n",
      "Epoch 15/100 | Batch 30/88 | train_accuracy: 9.938e-01, train_loss: 5.350e-02\n",
      "Epoch 15/100 | Batch 31/88 | train_accuracy: 9.940e-01, train_loss: 5.240e-02\n",
      "Epoch 15/100 | Batch 32/88 | train_accuracy: 9.941e-01, train_loss: 5.101e-02\n",
      "Epoch 15/100 | Batch 33/88 | train_accuracy: 9.943e-01, train_loss: 5.083e-02\n",
      "Epoch 15/100 | Batch 34/88 | train_accuracy: 9.945e-01, train_loss: 4.995e-02\n",
      "Epoch 15/100 | Batch 35/88 | train_accuracy: 9.929e-01, train_loss: 5.129e-02\n",
      "Epoch 15/100 | Batch 36/88 | train_accuracy: 9.913e-01, train_loss: 5.167e-02\n",
      "Epoch 15/100 | Batch 37/88 | train_accuracy: 9.916e-01, train_loss: 5.038e-02\n",
      "Epoch 15/100 | Batch 38/88 | train_accuracy: 9.918e-01, train_loss: 5.005e-02\n",
      "Epoch 15/100 | Batch 39/88 | train_accuracy: 9.920e-01, train_loss: 4.955e-02\n",
      "Epoch 15/100 | Batch 40/88 | train_accuracy: 9.906e-01, train_loss: 4.965e-02\n",
      "Epoch 15/100 | Batch 41/88 | train_accuracy: 9.909e-01, train_loss: 4.889e-02\n",
      "Epoch 15/100 | Batch 42/88 | train_accuracy: 9.896e-01, train_loss: 4.939e-02\n",
      "Epoch 15/100 | Batch 43/88 | train_accuracy: 9.898e-01, train_loss: 4.827e-02\n",
      "Epoch 15/100 | Batch 44/88 | train_accuracy: 9.901e-01, train_loss: 4.730e-02\n",
      "Epoch 15/100 | Batch 45/88 | train_accuracy: 9.903e-01, train_loss: 4.656e-02\n",
      "Epoch 15/100 | Batch 46/88 | train_accuracy: 9.905e-01, train_loss: 4.648e-02\n",
      "Epoch 15/100 | Batch 47/88 | train_accuracy: 9.907e-01, train_loss: 4.584e-02\n",
      "Epoch 15/100 | Batch 48/88 | train_accuracy: 9.896e-01, train_loss: 4.617e-02\n",
      "Epoch 15/100 | Batch 49/88 | train_accuracy: 9.898e-01, train_loss: 4.576e-02\n",
      "Epoch 15/100 | Batch 50/88 | train_accuracy: 9.900e-01, train_loss: 4.515e-02\n",
      "Epoch 15/100 | Batch 51/88 | train_accuracy: 9.902e-01, train_loss: 4.439e-02\n",
      "Epoch 15/100 | Batch 52/88 | train_accuracy: 9.892e-01, train_loss: 5.121e-02\n",
      "Epoch 15/100 | Batch 53/88 | train_accuracy: 9.882e-01, train_loss: 5.162e-02\n",
      "Epoch 15/100 | Batch 54/88 | train_accuracy: 9.884e-01, train_loss: 5.069e-02\n",
      "Epoch 15/100 | Batch 55/88 | train_accuracy: 9.886e-01, train_loss: 5.019e-02\n",
      "Epoch 15/100 | Batch 56/88 | train_accuracy: 9.877e-01, train_loss: 5.105e-02\n",
      "Epoch 15/100 | Batch 57/88 | train_accuracy: 9.879e-01, train_loss: 5.042e-02\n",
      "Epoch 15/100 | Batch 58/88 | train_accuracy: 9.881e-01, train_loss: 5.000e-02\n",
      "Epoch 15/100 | Batch 59/88 | train_accuracy: 9.873e-01, train_loss: 5.047e-02\n",
      "Epoch 15/100 | Batch 60/88 | train_accuracy: 9.875e-01, train_loss: 4.985e-02\n",
      "Epoch 15/100 | Batch 61/88 | train_accuracy: 9.877e-01, train_loss: 4.911e-02\n",
      "Epoch 15/100 | Batch 62/88 | train_accuracy: 9.879e-01, train_loss: 4.837e-02\n",
      "Epoch 15/100 | Batch 63/88 | train_accuracy: 9.881e-01, train_loss: 4.773e-02\n",
      "Epoch 15/100 | Batch 64/88 | train_accuracy: 9.883e-01, train_loss: 4.702e-02\n",
      "Epoch 15/100 | Batch 65/88 | train_accuracy: 9.875e-01, train_loss: 4.839e-02\n",
      "Epoch 15/100 | Batch 66/88 | train_accuracy: 9.858e-01, train_loss: 4.969e-02\n",
      "Epoch 15/100 | Batch 67/88 | train_accuracy: 9.860e-01, train_loss: 4.898e-02\n",
      "Epoch 15/100 | Batch 68/88 | train_accuracy: 9.853e-01, train_loss: 4.990e-02\n",
      "Epoch 15/100 | Batch 69/88 | train_accuracy: 9.855e-01, train_loss: 4.948e-02\n",
      "Epoch 15/100 | Batch 70/88 | train_accuracy: 9.857e-01, train_loss: 4.915e-02\n",
      "Epoch 15/100 | Batch 71/88 | train_accuracy: 9.859e-01, train_loss: 4.892e-02\n",
      "Epoch 15/100 | Batch 72/88 | train_accuracy: 9.861e-01, train_loss: 4.849e-02\n",
      "Epoch 15/100 | Batch 73/88 | train_accuracy: 9.863e-01, train_loss: 4.791e-02\n",
      "Epoch 15/100 | Batch 74/88 | train_accuracy: 9.865e-01, train_loss: 4.783e-02\n",
      "Epoch 15/100 | Batch 75/88 | train_accuracy: 9.867e-01, train_loss: 4.731e-02\n",
      "Epoch 15/100 | Batch 76/88 | train_accuracy: 9.860e-01, train_loss: 4.886e-02\n",
      "Epoch 15/100 | Batch 77/88 | train_accuracy: 9.862e-01, train_loss: 4.836e-02\n",
      "Epoch 15/100 | Batch 78/88 | train_accuracy: 9.864e-01, train_loss: 4.777e-02\n",
      "Epoch 15/100 | Batch 79/88 | train_accuracy: 9.866e-01, train_loss: 4.731e-02\n",
      "Epoch 15/100 | Batch 80/88 | train_accuracy: 9.859e-01, train_loss: 4.898e-02\n",
      "Epoch 15/100 | Batch 81/88 | train_accuracy: 9.846e-01, train_loss: 4.997e-02\n",
      "Epoch 15/100 | Batch 82/88 | train_accuracy: 9.848e-01, train_loss: 4.951e-02\n",
      "Epoch 15/100 | Batch 83/88 | train_accuracy: 9.849e-01, train_loss: 4.899e-02\n",
      "Epoch 15/100 | Batch 84/88 | train_accuracy: 9.851e-01, train_loss: 4.845e-02\n",
      "Epoch 15/100 | Batch 85/88 | train_accuracy: 9.853e-01, train_loss: 4.794e-02\n",
      "Epoch 15/100 | Batch 86/88 | train_accuracy: 9.855e-01, train_loss: 4.769e-02\n",
      "Epoch 15/100 | Batch 87/88 | train_accuracy: 9.842e-01, train_loss: 4.947e-02\n",
      "Epoch 15/100 | Batch 88/88 | train_accuracy: 9.843e-01, train_loss: 4.893e-02\n",
      "Epoch 15/100 | val_accuracy: 6.700e-01, val_loss: 1.270e+00\n",
      "====================\n",
      "Epoch 16/100 | Batch 1/88 | train_accuracy: 9.375e-01, train_loss: 7.194e-02\n",
      "Epoch 16/100 | Batch 2/88 | train_accuracy: 9.375e-01, train_loss: 1.091e-01\n",
      "Epoch 16/100 | Batch 3/88 | train_accuracy: 9.583e-01, train_loss: 7.347e-02\n",
      "Epoch 16/100 | Batch 4/88 | train_accuracy: 9.688e-01, train_loss: 5.667e-02\n",
      "Epoch 16/100 | Batch 5/88 | train_accuracy: 9.625e-01, train_loss: 1.368e-01\n",
      "Epoch 16/100 | Batch 6/88 | train_accuracy: 9.688e-01, train_loss: 1.166e-01\n",
      "Epoch 16/100 | Batch 7/88 | train_accuracy: 9.732e-01, train_loss: 1.040e-01\n",
      "Epoch 16/100 | Batch 8/88 | train_accuracy: 9.688e-01, train_loss: 1.036e-01\n",
      "Epoch 16/100 | Batch 9/88 | train_accuracy: 9.722e-01, train_loss: 9.431e-02\n",
      "Epoch 16/100 | Batch 10/88 | train_accuracy: 9.750e-01, train_loss: 8.765e-02\n",
      "Epoch 16/100 | Batch 11/88 | train_accuracy: 9.716e-01, train_loss: 8.563e-02\n",
      "Epoch 16/100 | Batch 12/88 | train_accuracy: 9.740e-01, train_loss: 7.938e-02\n",
      "Epoch 16/100 | Batch 13/88 | train_accuracy: 9.760e-01, train_loss: 7.827e-02\n",
      "Epoch 16/100 | Batch 14/88 | train_accuracy: 9.777e-01, train_loss: 7.388e-02\n",
      "Epoch 16/100 | Batch 15/88 | train_accuracy: 9.792e-01, train_loss: 7.009e-02\n",
      "Epoch 16/100 | Batch 16/88 | train_accuracy: 9.805e-01, train_loss: 6.673e-02\n",
      "Epoch 16/100 | Batch 17/88 | train_accuracy: 9.816e-01, train_loss: 6.786e-02\n",
      "Epoch 16/100 | Batch 18/88 | train_accuracy: 9.826e-01, train_loss: 6.475e-02\n",
      "Epoch 16/100 | Batch 19/88 | train_accuracy: 9.836e-01, train_loss: 6.426e-02\n",
      "Epoch 16/100 | Batch 20/88 | train_accuracy: 9.844e-01, train_loss: 6.126e-02\n",
      "Epoch 16/100 | Batch 21/88 | train_accuracy: 9.821e-01, train_loss: 6.365e-02\n",
      "Epoch 16/100 | Batch 22/88 | train_accuracy: 9.830e-01, train_loss: 6.175e-02\n",
      "Epoch 16/100 | Batch 23/88 | train_accuracy: 9.837e-01, train_loss: 6.017e-02\n",
      "Epoch 16/100 | Batch 24/88 | train_accuracy: 9.844e-01, train_loss: 5.955e-02\n",
      "Epoch 16/100 | Batch 25/88 | train_accuracy: 9.850e-01, train_loss: 5.830e-02\n",
      "Epoch 16/100 | Batch 26/88 | train_accuracy: 9.856e-01, train_loss: 5.689e-02\n",
      "Epoch 16/100 | Batch 27/88 | train_accuracy: 9.861e-01, train_loss: 5.519e-02\n",
      "Epoch 16/100 | Batch 28/88 | train_accuracy: 9.866e-01, train_loss: 5.359e-02\n",
      "Epoch 16/100 | Batch 29/88 | train_accuracy: 9.871e-01, train_loss: 5.205e-02\n",
      "Epoch 16/100 | Batch 30/88 | train_accuracy: 9.875e-01, train_loss: 5.044e-02\n",
      "Epoch 16/100 | Batch 31/88 | train_accuracy: 9.879e-01, train_loss: 4.914e-02\n",
      "Epoch 16/100 | Batch 32/88 | train_accuracy: 9.883e-01, train_loss: 4.919e-02\n",
      "Epoch 16/100 | Batch 33/88 | train_accuracy: 9.886e-01, train_loss: 4.793e-02\n",
      "Epoch 16/100 | Batch 34/88 | train_accuracy: 9.871e-01, train_loss: 5.120e-02\n",
      "Epoch 16/100 | Batch 35/88 | train_accuracy: 9.875e-01, train_loss: 5.008e-02\n",
      "Epoch 16/100 | Batch 36/88 | train_accuracy: 9.878e-01, train_loss: 4.879e-02\n",
      "Epoch 16/100 | Batch 37/88 | train_accuracy: 9.865e-01, train_loss: 5.164e-02\n",
      "Epoch 16/100 | Batch 38/88 | train_accuracy: 9.868e-01, train_loss: 5.098e-02\n",
      "Epoch 16/100 | Batch 39/88 | train_accuracy: 9.872e-01, train_loss: 4.971e-02\n",
      "Epoch 16/100 | Batch 40/88 | train_accuracy: 9.875e-01, train_loss: 4.865e-02\n",
      "Epoch 16/100 | Batch 41/88 | train_accuracy: 9.878e-01, train_loss: 4.759e-02\n",
      "Epoch 16/100 | Batch 42/88 | train_accuracy: 9.881e-01, train_loss: 4.674e-02\n",
      "Epoch 16/100 | Batch 43/88 | train_accuracy: 9.869e-01, train_loss: 4.735e-02\n",
      "Epoch 16/100 | Batch 44/88 | train_accuracy: 9.858e-01, train_loss: 4.746e-02\n",
      "Epoch 16/100 | Batch 45/88 | train_accuracy: 9.847e-01, train_loss: 5.024e-02\n",
      "Epoch 16/100 | Batch 46/88 | train_accuracy: 9.851e-01, train_loss: 4.923e-02\n",
      "Epoch 16/100 | Batch 47/88 | train_accuracy: 9.840e-01, train_loss: 5.213e-02\n",
      "Epoch 16/100 | Batch 48/88 | train_accuracy: 9.844e-01, train_loss: 5.109e-02\n",
      "Epoch 16/100 | Batch 49/88 | train_accuracy: 9.847e-01, train_loss: 5.027e-02\n",
      "Epoch 16/100 | Batch 50/88 | train_accuracy: 9.850e-01, train_loss: 4.955e-02\n",
      "Epoch 16/100 | Batch 51/88 | train_accuracy: 9.841e-01, train_loss: 5.116e-02\n",
      "Epoch 16/100 | Batch 52/88 | train_accuracy: 9.844e-01, train_loss: 5.045e-02\n",
      "Epoch 16/100 | Batch 53/88 | train_accuracy: 9.835e-01, train_loss: 5.205e-02\n",
      "Epoch 16/100 | Batch 54/88 | train_accuracy: 9.826e-01, train_loss: 5.445e-02\n",
      "Epoch 16/100 | Batch 55/88 | train_accuracy: 9.830e-01, train_loss: 5.372e-02\n",
      "Epoch 16/100 | Batch 56/88 | train_accuracy: 9.833e-01, train_loss: 5.324e-02\n",
      "Epoch 16/100 | Batch 57/88 | train_accuracy: 9.836e-01, train_loss: 5.273e-02\n",
      "Epoch 16/100 | Batch 58/88 | train_accuracy: 9.838e-01, train_loss: 5.359e-02\n",
      "Epoch 16/100 | Batch 59/88 | train_accuracy: 9.841e-01, train_loss: 5.306e-02\n",
      "Epoch 16/100 | Batch 60/88 | train_accuracy: 9.833e-01, train_loss: 5.363e-02\n",
      "Epoch 16/100 | Batch 61/88 | train_accuracy: 9.836e-01, train_loss: 5.306e-02\n",
      "Epoch 16/100 | Batch 62/88 | train_accuracy: 9.819e-01, train_loss: 5.717e-02\n",
      "Epoch 16/100 | Batch 63/88 | train_accuracy: 9.812e-01, train_loss: 5.774e-02\n",
      "Epoch 16/100 | Batch 64/88 | train_accuracy: 9.805e-01, train_loss: 5.847e-02\n",
      "Epoch 16/100 | Batch 65/88 | train_accuracy: 9.808e-01, train_loss: 5.775e-02\n",
      "Epoch 16/100 | Batch 66/88 | train_accuracy: 9.811e-01, train_loss: 5.764e-02\n",
      "Epoch 16/100 | Batch 67/88 | train_accuracy: 9.813e-01, train_loss: 5.752e-02\n",
      "Epoch 16/100 | Batch 68/88 | train_accuracy: 9.789e-01, train_loss: 6.620e-02\n",
      "Epoch 16/100 | Batch 69/88 | train_accuracy: 9.792e-01, train_loss: 6.560e-02\n",
      "Epoch 16/100 | Batch 70/88 | train_accuracy: 9.795e-01, train_loss: 6.499e-02\n",
      "Epoch 16/100 | Batch 71/88 | train_accuracy: 9.798e-01, train_loss: 6.442e-02\n",
      "Epoch 16/100 | Batch 72/88 | train_accuracy: 9.800e-01, train_loss: 6.371e-02\n",
      "Epoch 16/100 | Batch 73/88 | train_accuracy: 9.803e-01, train_loss: 6.333e-02\n",
      "Epoch 16/100 | Batch 74/88 | train_accuracy: 9.806e-01, train_loss: 6.328e-02\n",
      "Epoch 16/100 | Batch 75/88 | train_accuracy: 9.808e-01, train_loss: 6.332e-02\n",
      "Epoch 16/100 | Batch 76/88 | train_accuracy: 9.811e-01, train_loss: 6.283e-02\n",
      "Epoch 16/100 | Batch 77/88 | train_accuracy: 9.813e-01, train_loss: 6.215e-02\n",
      "Epoch 16/100 | Batch 78/88 | train_accuracy: 9.816e-01, train_loss: 6.152e-02\n",
      "Epoch 16/100 | Batch 79/88 | train_accuracy: 9.818e-01, train_loss: 6.085e-02\n",
      "Epoch 16/100 | Batch 80/88 | train_accuracy: 9.820e-01, train_loss: 6.019e-02\n",
      "Epoch 16/100 | Batch 81/88 | train_accuracy: 9.823e-01, train_loss: 6.022e-02\n",
      "Epoch 16/100 | Batch 82/88 | train_accuracy: 9.825e-01, train_loss: 6.008e-02\n",
      "Epoch 16/100 | Batch 83/88 | train_accuracy: 9.827e-01, train_loss: 5.970e-02\n",
      "Epoch 16/100 | Batch 84/88 | train_accuracy: 9.829e-01, train_loss: 5.904e-02\n",
      "Epoch 16/100 | Batch 85/88 | train_accuracy: 9.831e-01, train_loss: 5.840e-02\n",
      "Epoch 16/100 | Batch 86/88 | train_accuracy: 9.826e-01, train_loss: 5.955e-02\n",
      "Epoch 16/100 | Batch 87/88 | train_accuracy: 9.828e-01, train_loss: 5.898e-02\n",
      "Epoch 16/100 | Batch 88/88 | train_accuracy: 9.829e-01, train_loss: 5.833e-02\n",
      "Epoch 16/100 | val_accuracy: 7.200e-01, val_loss: 1.097e+00\n",
      "====================\n",
      "Early Stopped\n"
     ]
    }
   ],
   "source": [
    "net = train(\n",
    "    model=net,\n",
    "    train_dataloader=DataLoader(dataset=train_dataset, batch_size=16, shuffle=True),\n",
    "    val_dataloader=DataLoader(dataset=valid_dataset, batch_size=4, shuffle=False),\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=100,\n",
    "    patience=10,\n",
    "    tolerance=0.,\n",
    "    checkpoint_dir='.checkpoints',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63262f",
   "metadata": {},
   "source": [
    "# 3. Evaluate your model using the developed software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4437136c",
   "metadata": {},
   "source": [
    "Define the `predict` function to evaluate the model on test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d337fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e9df13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: nn.Module, dataloader: DataLoader) -> pd.DataFrame:\n",
    "    model.eval()\n",
    "\n",
    "    filenames = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, fnames in dataloader:\n",
    "            images = images.to(next(model.parameters()).device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            filenames.extend(fnames)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    prediction_table = pd.DataFrame(\n",
    "        data={'image': filenames, 'label': predictions}\n",
    "    )\n",
    "    prediction_table.to_csv(\n",
    "        f'{dt.datetime.now().strftime(r\"%Y%m%d%H%M%S\")}.csv', \n",
    "        header=False, \n",
    "        index=False\n",
    "    )\n",
    "    return prediction_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5522a2b2",
   "metadata": {},
   "source": [
    "Load the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "687038bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint: str = '.checkpoints/epoch16.pt'\n",
    "\n",
    "trained_model: NeuralNet = torch.load(last_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee238394",
   "metadata": {},
   "source": [
    "Evaluate the model on test dataset. A `.csv` file is output to load to the developed software:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bdacebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DogHearUnlabeledDataset(data_root='Dog_heart/Test')\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=16, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "563674d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1804.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1810.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1838.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1623.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1637.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1955.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1969.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1835.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1821.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1809.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image  label\n",
       "0    1804.png      2\n",
       "1    1810.png      0\n",
       "2    1838.png      0\n",
       "3    1623.png      2\n",
       "4    1637.png      2\n",
       "..        ...    ...\n",
       "395  1955.png      0\n",
       "396  1969.png      1\n",
       "397  1835.png      1\n",
       "398  1821.png      0\n",
       "399  1809.png      0\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model=trained_model, dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fdec95",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/hiepdang-ml/dog_heart_classification/blob/master/test.png?raw=true\" alt=\"PredictionImage\" style=\"width:50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5846bc",
   "metadata": {},
   "source": [
    "# 4. Compare results with [RVT paper](https://www.nature.com/articles/s41598-023-50063-x). Requirement: performance is better than VGG16: 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d454ff",
   "metadata": {},
   "source": [
    "We got `71%` accuracy on test dataset, which is better than `VGG16`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f12835",
   "metadata": {},
   "source": [
    "# 5. Write a four-page paper report using the shared LaTex template. Upload your paper to ResearchGate or Arxiv, and put your paper link and GitHub weight link here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1379e9f",
   "metadata": {},
   "source": [
    "Paper: `...`\n",
    "\n",
    "Source code: https://github.com/hiepdang-ml/dog_heart_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476372c",
   "metadata": {},
   "source": [
    "# 6. Grading rubric\n",
    "\n",
    "(1). Code ------- 20 points (you also need to upload your final model as a pt file)\n",
    "\n",
    "(2). Grammer ---- 20 points\n",
    "\n",
    "(3). Introduction & related work --- 10 points\n",
    "\n",
    "\n",
    "(4). Method  ---- 20 points\n",
    "\n",
    "(5). Results ---- 20 points\n",
    "\n",
    "     > = 70 % -->10 points\n",
    "     < 50 % -->0 points\n",
    "     >= 50 % & < 70% --> 0.5 point/percent\n",
    "     \n",
    "\n",
    "(6). Discussion - 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a3a6e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
